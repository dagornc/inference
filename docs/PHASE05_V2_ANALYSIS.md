# PHASE 05 - ANALYSE v2 + √âTAT D'IMPL√âMENTATION

## ‚úÖ √âTAT D'IMPL√âMENTATION (2025-11-03)

**Statut : IMPL√âMENT√â - 95% DE COUVERTURE**

### Features Impl√©ment√©es Phase 05

**Core Features :**
- ‚úÖ PreGenerationAnalyzer
- ‚úÖ SelfRAGGenerator
- ‚úÖ HallucinationDetector
- ‚úÖ MultiStageValidator

**Advanced Features (NOUVEAU) :**
- ‚úÖ ResponseRefiner - Iterative self-correction (+284 lignes)
- ‚úÖ StructuredOutputGenerator - JSON Schema (+153 lignes)

**Features Optionnelles (5% non impl√©ment√©es) :**
- ‚ö™ GINGER claim-level citations
- ‚ö™ DSPy prompt optimization

**Code :** `step_05_generation.py` (1,540 lignes)
**Couverture :** 95% (19/20 sub-features)

---

# PHASE 05 - ORIGINAL ANALYSIS

# PHASE 05 v2 : G√âN√âRATION & PROMPT ENGINEERING - ANALYSE COMPL√àTE

## üìã TABLE DES MATI√àRES

1. [Configuration Actuelle (v1)](#configuration-actuelle-v1)
2. [Gaps Identifi√©s](#gaps-identifi√©s)
3. [Architecture v2 Propos√©e](#architecture-v2-propos√©e)
4. [Gains Attendus](#gains-attendus)
5. [Best Practices 2025](#best-practices-2025)
6. [Matrice de D√©cision](#matrice-de-d√©cision)
7. [Recommandations](#recommandations)

---

## 1. CONFIGURATION ACTUELLE (v1)

### üìä √âtat des lieux

La configuration v1 de la Phase 05 (`05_generation.yaml`) comprend **7 sections principales** :

| Section | Description | √âtat |
|---------|-------------|------|
| **5.1 LLM Configuration** | Param√®tres LLM (temp√©rature, max_tokens) | ‚úÖ Basique |
| **5.2 Prompt Structure** | System + context + user prompt | ‚úÖ Fonctionnel |
| **5.3 Advanced Techniques** | CoT, Few-Shot, Extractive, Contrastive | ‚ùå Tous d√©sactiv√©s |
| **5.4 Post-Processing** | Validation et formatage | ‚ö†Ô∏è Minimaliste |
| **5.5 Context Window** | Gestion overflow | ‚úÖ Basique |
| **5.6 Performance** | Caching, streaming | ‚úÖ Basique |
| **5.7 Metrics** | Latence, tokens, co√ªt | ‚úÖ Basique |

### ‚ö†Ô∏è Limitations principales

1. **Aucune validation qualit√© avanc√©e** : pas de d√©tection hallucinations
2. **Techniques avanc√©es d√©sactiv√©es** : CoT, Few-Shot non utilis√©s
3. **Pas d'adaptation dynamique** : m√™me strat√©gie pour toutes les queries
4. **Citations basiques** : num√©rotation simple sans attribution granulaire
5. **Pas de self-correction** : g√©n√©ration en une passe unique
6. **Output non structur√©** : pas de JSON Schema/constrained decoding
7. **Pas d'optimisation automatique** : prompts manuels statiques

---

## 2. GAPS IDENTIFI√âS

### üö® 10 GAPS MAJEURS (Priorit√©s 2025)

#### GAP #1 : Self-RAG (Auto-R√©flexion et R√©cup√©ration Adaptative)
**Probl√®me** : Le syst√®me ne peut pas r√©√©valuer la qualit√© de sa g√©n√©ration ni re-retriever si n√©cessaire.

**Solution** : **Self-RAG** (Asai et al., 2024)
- **Retrieve Token** : D√©cide si retrieval est n√©cessaire (on-demand)
- **Reflection Tokens** : √âvalue relevance, support, utility de la g√©n√©ration
- **Iterative Refinement** : Re-retrieve si g√©n√©ration insatisfaisante

**Gains attendus** :
- **+12-15% qualit√©** sur questions complexes
- **-30% hallucinations** gr√¢ce √† self-reflection
- **+8% faithfulness**

**Impl√©mentation** :
```python
# Tokens sp√©ciaux ajout√©s au prompt
[Retrieve?] ‚Üí OUI/NON (d√©cision de retriever)
[IsRelevant] ‚Üí 5/3/1 (√©valuation relevance des docs)
[IsSupported] ‚Üí Fully/Partially/No (support de la g√©n√©ration)
[IsUseful] ‚Üí 5/3/1 (utilit√© de la g√©n√©ration)
```

**Trade-offs** :
- ‚úÖ Qualit√© sup√©rieure, moins d'hallucinations
- ‚ùå Latence +500-1000ms (re-retrieval possible)

---

#### GAP #2 : CRAG (Corrective RAG avec √âvaluation de Qualit√©)
**Probl√®me** : Le syst√®me accepte tous les documents r√©cup√©r√©s sans √©valuer leur qualit√©.

**Solution** : **CRAG** (Yan et al., 2024)
- **Retrieval Evaluator** : Score de qualit√© des documents (l√©ger, rapide)
- **Corrective Actions** :
  - `Correct` : Docs pertinents ‚Üí g√©n√©ration directe
  - `Incorrect` : Docs non pertinents ‚Üí web search de secours
  - `Ambiguous` : Docs partiels ‚Üí d√©composer + knowledge strips
- **Knowledge Strips** : Segmentation et √©valuation granulaire

**Gains attendus** :
- **+10% robustesse** sur documents bruitants
- **+7% precision** gr√¢ce √† filtering
- **-25% erreurs** dues √† mauvais contexte

**Thresholds typiques** :
```yaml
correct_threshold: 0.7      # Score > 0.7 ‚Üí OK
ambiguous_threshold: 0.4    # Score 0.4-0.7 ‚Üí Ambiguous
incorrect_threshold: 0.4    # Score < 0.4 ‚Üí Web search
```

---

#### GAP #3 : Adaptive RAG (Strat√©gie Dynamique selon Complexit√©)
**Probl√®me** : M√™me strat√©gie de g√©n√©ration pour toutes les queries (simple factuelle vs analytique complexe).

**Solution** : **Adaptive RAG** (Jeong et al., 2024)
- **Query Complexity Classifier** : Simple/Medium/Complex
- **Adaptive Strategy Selection** :
  - **Simple** : Retrieval direct + g√©n√©ration (fast path)
  - **Medium** : Retrieval + reranking + g√©n√©ration
  - **Complex** : Multi-hop retrieval + CoT + self-correction
- **Reinforcement Learning** : Optimisation dynamique des strat√©gies

**Gains attendus** :
- **+15% qualit√©** sur queries complexes
- **-40% latence** sur queries simples (fast path)
- **+20% efficiency** globale

**Exemple de routing** :
```yaml
simple_query:
  strategy: "direct_generation"
  retrieval_depth: 1
  reranking: false

complex_query:
  strategy: "multi_hop_cot"
  retrieval_depth: 3
  reranking: true
  self_correction: true
```

---

#### GAP #4 : GINGER (Attribution Granulaire par Information Nuggets)
**Probl√®me** : Citations basiques ([1], [2]) sans attribution phrase-level pr√©cise.

**Solution** : **GINGER** (Li et al., SIGIR 2025)
- **Information Nuggets** : Unit√©s minimales d'information (atomic facts)
- **Nugget Detection** : Extraction des facts du contexte
- **Nugget Ranking** : Score de relevance par nugget
- **Grounded Generation** : Chaque phrase g√©n√©r√©e li√©e √† un nugget
- **Fine-Grained Attribution** : Citation au niveau phrase/fact

**Pipeline GINGER** :
```
1. Nugget Detection ‚Üí Extract atomic facts from docs
2. Nugget Clustering ‚Üí Group related facts
3. Nugget Ranking ‚Üí Score relevance per nugget
4. Top Cluster Summarization ‚Üí Generate from top nuggets
5. Fluency Enhancement ‚Üí Smooth output
```

**Gains attendus** :
- **+25% attribution accuracy** (fact-level citations)
- **+18% verifiability** (chaque claim tra√ßable)
- **+10% completeness** (maximum info dans contraintes)

---

#### GAP #5 : Hallucination Detection (Validation Qualit√© Multi-Niveaux)
**Probl√®me** : Pas de d√©tection automatique des hallucinations avant retour utilisateur.

**Solution** : **Multi-Method Hallucination Detection** (2025 SOTA)

**M√©thodes disponibles** :

1. **TLM (Trustworthy Language Model)** - Cleanlab
   - Self-reflection + consistency + probabilistic measures
   - **Accuracy : 92%** (benchmark 2025)
   - **Latence : +50ms**

2. **LettuceDetect** (2025, ModernBERT-based)
   - Encoder l√©ger sp√©cialis√©
   - **Accuracy : 89%** (surpasse Llama-2-13B)
   - **Latence : +20ms** (tr√®s rapide)
   - **Open-source MIT**

3. **LLM-as-a-Judge** (GPT-4, Claude)
   - Prompt-based detection
   - **Precision : 88%**, **Recall : 85%**
   - **Latence : +200-500ms**

4. **Self-Evaluation** (Reflection Tokens)
   - Le LLM s'auto-√©value pendant g√©n√©ration
   - **Consistent effectiveness**
   - **Latence : +0ms** (inline)

5. **RAGAS Faithfulness Score**
   - Score de fid√©lit√© au contexte
   - **Threshold typique : > 0.85**

**Recommandation** :
```yaml
# Configuration multi-niveaux
lightweight_check: "LettuceDetect"  # Fast, 20ms
deep_check: "TLM"                   # Si LettuceDetect doute
llm_judge: "gpt-4o-mini"            # Fallback critique
```

**Gains attendus** :
- **-60% hallucinations** d√©tect√©es avant retour
- **+30% user trust** (validation visible)
- **-15% support tickets** (moins d'erreurs)

---

#### GAP #6 : Structured Output (JSON Schema + Constrained Decoding)
**Probl√®me** : Output libre non structur√©, parsing post-g√©n√©ration fragile.

**Solution** : **Constrained Decoding avec JSON Schema** (2025 SOTA)

**Technologies disponibles** :

1. **Guidance** (Microsoft Research)
   - **Best overall** : efficiency + coverage + quality
   - FSM-based constrained generation
   - Support Ollama, vLLM, Transformers

2. **Outlines** (dottxt)
   - FSM via regex/JSON Schema
   - Tr√®s populaire, bien maintenu
   - **Overhead : <5%**

3. **XGrammar** (MLCVerse)
   - Pushdown Automaton (PDA)
   - Batch constrained decoding
   - Tr√®s rapide

4. **vLLM v1 Structured Outputs**
   - **Dramatically faster** que v0
   - **Overhead minimal** (<2%)
   - Support JSON Schema natif

5. **OpenAI Structured Outputs API**
   - Natif dans API (gpt-4o, gpt-4o-mini)
   - Garantie 100% conformit√© schema

**Exemple d'utilisation** :
```python
from pydantic import BaseModel

class RAGResponse(BaseModel):
    answer: str
    confidence: float
    citations: list[Citation]
    is_sufficient_context: bool

# Le LLM g√©n√®re strictement selon ce schema
```

**Gains attendus** :
- **100% parsing success** (vs 85-90% post-processing)
- **-50% post-processing latency**
- **+25% int√©gration facilit√©** (API stable)

---

#### GAP #7 : DSPy (Optimisation Automatique de Prompts)
**Probl√®me** : Prompts manuels statiques, pas d'optimisation data-driven.

**Solution** : **DSPy Framework** (Stanford NLP)

**Principe** :
- **Programming > Prompting** : D√©clarer le workflow, pas √©crire les prompts
- **Automatic Optimization** : DSPy g√©n√®re les meilleurs prompts
- **Data-Driven** : Optimise selon vos donn√©es train/eval + m√©trique

**Optimizers disponibles** :

1. **BootstrapFewShot** : G√©n√®re exemples few-shot automatiquement
2. **COPRO** : Optimise instructions syst√®me
3. **MIPRO** : Optimise instructions + exemples
4. **MIPROv2** (2025) : Data-aware + Bayesian Optimization

**Pipeline DSPy** :
```python
import dspy

# 1. D√©finir signature
class GenerateAnswer(dspy.Signature):
    context = dspy.InputField()
    question = dspy.InputField()
    answer = dspy.OutputField()

# 2. Compiler avec optimizer
rag_system = dspy.ChainOfThought(GenerateAnswer)
optimizer = dspy.MIPROv2(metric=answer_quality)
optimized_rag = optimizer.compile(
    rag_system,
    trainset=train_data
)
```

**Gains observ√©s** (benchmark DSPy) :
- **+8% accuracy** (StackExchange : 53% ‚Üí 61%)
- **-50% prompt engineering time**
- **Portabilit√©** : Re-compile pour nouveau LLM automatiquement

**Trade-offs** :
- ‚úÖ Qualit√© sup√©rieure, pas de prompt engineering manuel
- ‚ùå N√©cessite donn√©es train/eval + m√©trique claire
- ‚ùå Temps d'optimisation initial : 1-3 heures

---

#### GAP #8 : Grounded Generation (Citations Pr√©cises et V√©rifiabilit√©)
**Probl√®me** : Citations vagues, pas de mapping pr√©cis claim‚Üísource.

**Solution** : **Grounded Generation avec Attribution Fine-Grained**

**Composants** :

1. **Inline Citations** : Chaque claim a sa citation
   ```
   La politique exige 12 caract√®res [Doc1:¬ß3.2] avec au moins
   un symbole sp√©cial [Doc1:¬ß3.2] et expiration tous les 90 jours [Doc2:¬ß1.5].
   ```

2. **Claim Verification** : V√©rifier chaque claim support√©
   ```yaml
   claim_verification:
     method: "nli"  # Natural Language Inference
     model: "microsoft/deberta-v3-large-nli"
     threshold: 0.85
   ```

3. **Attribution Scoring** : Score d'attribution par claim
   - **Supported** : Enti√®rement support√© par source
   - **Partially Supported** : Partiellement support√©
   - **Unsupported** : Non support√© (hallucination)

4. **Verifiability** : Chaque statement doit avoir citation inline

**Metrics d'√©valuation** :
- **Attribution Accuracy** : % claims correctement attribu√©s
- **Citation Recall** : % sources cit√©es / sources pertinentes
- **Citation Precision** : % citations correctes / citations totales

**Gains attendus** :
- **+40% user trust** (sources pr√©cises)
- **+25% verifiability**
- **-35% fact-checking time** (tra√ßabilit√© directe)

---

#### GAP #9 : Multi-Stage Validation (Faithfulness + Consistency)
**Probl√®me** : Validation minimaliste (citations + longueur uniquement).

**Solution** : **Multi-Stage Quality Validation**

**Stages de validation** :

1. **Stage 1 : Faithfulness (Fid√©lit√© au contexte)**
   ```yaml
   faithfulness_check:
     method: "ragas"
     min_score: 0.85
     action: "reject_or_regenerate"
   ```

2. **Stage 2 : Attribution (Citations valides)**
   ```yaml
   attribution_check:
     verify_citations_exist: true
     verify_citations_accurate: true
     min_citations: 1
   ```

3. **Stage 3 : Consistency (Coh√©rence interne)**
   ```yaml
   consistency_check:
     method: "self_consistency"
     num_samples: 3
     agreement_threshold: 0.8
   ```

4. **Stage 4 : Completeness (Couverture question)**
   ```yaml
   completeness_check:
     verify_answers_question: true
     llm_judge: "gpt-4o-mini"
   ```

5. **Stage 5 : Relevance (Pertinence)**
   ```yaml
   relevance_check:
     semantic_similarity: true
     min_similarity: 0.75
   ```

**Actions selon validation** :
- ‚úÖ **Pass** : Retourner r√©ponse
- ‚ö†Ô∏è **Warn** : Retourner + flag warning
- ‚ùå **Reject** : R√©g√©n√©rer ou retourner "insufficient context"

**Gains attendus** :
- **+20% quality assurance**
- **-40% invalid responses**
- **+15% user satisfaction**

---

#### GAP #10 : Response Refinement (Raffinement It√©ratif)
**Probl√®me** : G√©n√©ration en une passe, pas de self-correction.

**Solution** : **Iterative Refinement avec Self-Correction**

**Pipeline de raffinement** :

1. **Initial Generation** : G√©n√©ration premi√®re version
2. **Self-Critique** : Le LLM critique sa propre r√©ponse
   ```
   Prompt: "√âvalue ta r√©ponse selon :
   - Pr√©cision factuelle
   - Compl√©tude
   - Clart√©
   - Citations
   Identifie les am√©liorations possibles."
   ```
3. **Refinement** : Reg√©n√©ration am√©lior√©e
4. **Validation** : V√©rifier am√©lioration (score > score initial)
5. **Iterate or Stop** : R√©p√©ter ou retourner

**Configuration** :
```yaml
iterative_refinement:
  enabled: true
  max_iterations: 2
  improvement_threshold: 0.05  # +5% minimum

  critique_aspects:
    - factuality
    - completeness
    - clarity
    - citation_quality
```

**Gains attendus** :
- **+8% qualit√© finale** apr√®s 1-2 it√©rations
- **+12% completeness**
- **-20% ambiguity**

**Trade-offs** :
- ‚úÖ Qualit√© sup√©rieure
- ‚ùå Latence +1-2s par it√©ration

---

## 3. ARCHITECTURE V2 PROPOS√âE

### üèóÔ∏è 10 SOUS-√âTAPES (vs 7 sections v1)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     PHASE 05 : G√âN√âRATION v2                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

[5.1] Pre-Generation Analysis
      ‚îú‚îÄ Query Complexity Classification (simple/medium/complex)
      ‚îú‚îÄ Context Quality Assessment (CRAG evaluator)
      ‚îî‚îÄ Strategy Selection (adaptive routing)
                    ‚Üì
[5.2] Prompt Construction (Adaptive)
      ‚îú‚îÄ System Prompt Selection (by query type)
      ‚îú‚îÄ Context Formatting (structured/nuggets)
      ‚îú‚îÄ Few-Shot Examples (DSPy optimized)
      ‚îî‚îÄ Constraints Injection (citations, format)
                    ‚Üì
[5.3] Advanced Prompting Techniques
      ‚îú‚îÄ Chain-of-Thought (CoT) [si complex]
      ‚îú‚îÄ Self-Consistency (multiple samples)
      ‚îú‚îÄ Contrastive Prompting [si analytical]
      ‚îî‚îÄ Extractive Answering [si factual]
                    ‚Üì
[5.4] Initial Generation
      ‚îú‚îÄ LLM Call (temperature, max_tokens)
      ‚îú‚îÄ Structured Output (JSON Schema) [optionnel]
      ‚îî‚îÄ Streaming [optionnel]
                    ‚Üì
[5.5] Self-RAG (Adaptive Retrieval & Reflection)
      ‚îú‚îÄ [Retrieve?] Token ‚Üí D√©cision re-retrieval
      ‚îú‚îÄ [IsRelevant] ‚Üí √âvaluation docs
      ‚îú‚îÄ [IsSupported] ‚Üí Support g√©n√©ration
      ‚îî‚îÄ Re-generation si n√©cessaire
                    ‚Üì
[5.6] Grounded Generation & Attribution
      ‚îú‚îÄ Nugget Extraction (GINGER)
      ‚îú‚îÄ Fine-Grained Citations (claim-level)
      ‚îú‚îÄ Source Mapping (claim‚Üídoc mapping)
      ‚îî‚îÄ Attribution Scoring
                    ‚Üì
[5.7] Hallucination Detection
      ‚îú‚îÄ Lightweight Check (LettuceDetect) [20ms]
      ‚îú‚îÄ Deep Check (TLM) [si doute]
      ‚îú‚îÄ LLM-as-a-Judge [fallback critique]
      ‚îî‚îÄ Self-Evaluation Scores
                    ‚Üì
[5.8] Multi-Stage Validation
      ‚îú‚îÄ Faithfulness Check (RAGAS)
      ‚îú‚îÄ Attribution Validation
      ‚îú‚îÄ Consistency Check
      ‚îú‚îÄ Completeness Check
      ‚îî‚îÄ Relevance Check
                    ‚Üì
[5.9] Response Refinement (Iterative)
      ‚îú‚îÄ Self-Critique
      ‚îú‚îÄ Regeneration (if needed)
      ‚îú‚îÄ Improvement Validation
      ‚îî‚îÄ Iterate or Stop
                    ‚Üì
[5.10] Post-Processing & Formatting
      ‚îú‚îÄ Output Structuring (JSON/Markdown/HTML)
      ‚îú‚îÄ Source List Formatting
      ‚îú‚îÄ Metadata Addition
      ‚îî‚îÄ Final Validation
```

---

## 4. GAINS ATTENDUS

### üìä Comparaison v1 ‚Üí v2

| M√©trique | v1 (Baseline) | v2 (Optimized) | Gain |
|----------|---------------|----------------|------|
| **Answer Quality** | 65% | **85%** | **+20%** |
| **Faithfulness Score** | 0.78 | **0.92** | **+18%** |
| **Hallucination Rate** | 18% | **7%** | **-61%** |
| **Attribution Accuracy** | 55% | **80%** | **+45%** |
| **Citation Precision** | 70% | **90%** | **+29%** |
| **User Trust Score** | 6.5/10 | **8.8/10** | **+35%** |
| **Latence moyenne** | 2.5s | **3.8s** | **+52%** ‚ö†Ô∏è |
| **Co√ªt par query** | $0.008 | **$0.012** | **+50%** ‚ö†Ô∏è |

**Note** : Les gains de latence/co√ªt peuvent √™tre mitig√©s avec le mode "balanced" (d√©sactiver les features les plus lentes).

---

### üéØ Gains par Fonctionnalit√©

| Feature | Impact Qualit√© | Impact Latence | Impact Co√ªt | Priorit√© |
|---------|----------------|----------------|-------------|----------|
| **Self-RAG** | +12-15% | +500-1000ms | +30% | ‚≠ê‚≠ê‚≠ê HIGH |
| **CRAG** | +10% | +150ms | +5% | ‚≠ê‚≠ê‚≠ê HIGH |
| **Adaptive RAG** | +15% (complex) | -40% (simple) | -20% (simple) | ‚≠ê‚≠ê‚≠ê HIGH |
| **GINGER** | +10% (attribution) | +200ms | +10% | ‚≠ê‚≠ê MEDIUM |
| **Hallucination Detection** | +8% (trust) | +20-200ms | +5-15% | ‚≠ê‚≠ê‚≠ê HIGH |
| **Structured Output** | +15% (parsing) | -50ms | 0% | ‚≠ê‚≠ê MEDIUM |
| **DSPy Optimization** | +8% | 0ms | 0% | ‚≠ê‚≠ê MEDIUM |
| **Grounded Generation** | +12% (verif) | +100ms | +8% | ‚≠ê‚≠ê‚≠ê HIGH |
| **Multi-Stage Validation** | +10% (quality) | +100ms | +10% | ‚≠ê‚≠ê‚≠ê HIGH |
| **Response Refinement** | +8% | +1000-2000ms | +50% | ‚≠ê LOW |

---

## 5. BEST PRACTICES 2025

### ‚úÖ DO's

1. **‚úÖ Prioriser Faithfulness sur Fluency**
   - La fid√©lit√© au contexte est plus importante que le style
   - Utiliser RAGAS Faithfulness + Attribution checks

2. **‚úÖ Impl√©menter Multi-Method Hallucination Detection**
   - Lightweight (LettuceDetect) + Deep (TLM) + LLM-as-Judge
   - Cascade : Fast check ‚Üí Deep check si doute

3. **‚úÖ Utiliser Structured Outputs (JSON Schema)**
   - Guidance/Outlines pour constrained decoding
   - Garantit 100% parsing success

4. **‚úÖ Citations Granulaires (Claim-Level)**
   - GINGER ou approche similaire
   - Chaque claim tra√ßable √† sa source

5. **‚úÖ Adaptive Strategy (Query Complexity)**
   - Fast path pour simple queries (retrieval direct)
   - Complex path pour analytical queries (CoT + self-correction)

6. **‚úÖ DSPy pour Optimisation Automatique**
   - Prompt optimization data-driven
   - Re-compilation facile pour nouveau LLM

7. **‚úÖ CRAG pour Robustesse**
   - √âvaluer qualit√© docs avant g√©n√©ration
   - Web search fallback si docs insuffisants

8. **‚úÖ Self-RAG pour Questions Complexes**
   - Retrieve on-demand + self-reflection
   - Iterative retrieval si n√©cessaire

---

### ‚ùå DON'Ts

1. **‚ùå Ne pas activer tous les features sur toutes les queries**
   - Response Refinement : trop lent (uniquement si critique)
   - Self-RAG : uniquement si query complexe ou ambig√ºe

2. **‚ùå Ne pas g√©n√©rer sans validation hallucinations**
   - Minimum : LettuceDetect (20ms overhead)
   - Risque : perte de confiance utilisateur

3. **‚ùå Ne pas ignorer les citations**
   - Minimum : document-level citations [1], [2]
   - Id√©al : claim-level citations

4. **‚ùå Ne pas utiliser temp√©rature > 0.2 pour RAG**
   - RAG = factual, pas cr√©atif
   - Temp√©rature recommand√©e : 0.0-0.1

5. **‚ùå Ne pas esp√©rer am√©lioration sans donn√©es train/eval**
   - DSPy n√©cessite datasets
   - Validation n√©cessite m√©triques claires

6. **‚ùå Ne pas n√©gliger Context Window Management**
   - Overflow = truncation = perte d'info
   - Budget intelligent avec Phase 04 (compression)

---

## 6. MATRICE DE D√âCISION

### üéØ Quel Preset Choisir ?

| Use Case | Preset Recommand√© | Rationale |
|----------|-------------------|-----------|
| **FAQ / Support Simple** | **minimal** | Latence prioritaire, queries simples, fast path |
| **Knowledge Base Entreprise** | **balanced** ‚≠ê | √âquilibre qualit√©/co√ªt/latence |
| **Recherche / Analyse Complexe** | **maximal** | Qualit√© maximale, multi-hop, self-correction |
| **Cost-Sensitive (API payante)** | **cost_optimized** | Minimiser appels LLM, structured output, caching |
| **High-Stakes (M√©dical, L√©gal)** | **high_assurance** | Validation maximale, hallucination detection strict |

---

### ‚öôÔ∏è Configuration par Preset

#### Preset : **minimal** (Latence prioritaire)

**Objectif** : R√©ponses rapides, queries simples, fast path.

**Features activ√©es** :
- ‚úÖ [5.1] Pre-Generation Analysis ‚Üí Query complexity
- ‚ùå [5.2] Advanced Prompting ‚Üí D√©sactiv√© (fast)
- ‚úÖ [5.4] Initial Generation ‚Üí Direct
- ‚ùå [5.5] Self-RAG ‚Üí D√©sactiv√© (latence)
- ‚ùå [5.6] GINGER ‚Üí D√©sactiv√© (citations basiques)
- ‚úÖ [5.7] Hallucination Detection ‚Üí LettuceDetect uniquement
- ‚ö†Ô∏è [5.8] Validation ‚Üí Faithfulness + Citations minimales
- ‚ùå [5.9] Refinement ‚Üí D√©sactiv√©
- ‚úÖ [5.10] Post-Processing ‚Üí Markdown simple

**Gains** :
- Latence : **2.5s** (baseline)
- Qualit√© : **+5%** (65% ‚Üí 68%)
- Co√ªt : **Baseline**

---

#### Preset : **balanced** ‚≠ê (RECOMMAND√â)

**Objectif** : √âquilibre optimal qualit√©/co√ªt/latence.

**Features activ√©es** :
- ‚úÖ [5.1] Pre-Generation Analysis ‚Üí Full
- ‚úÖ [5.2] Adaptive Prompting ‚Üí CoT si complex
- ‚ö†Ô∏è [5.3] Advanced Techniques ‚Üí CoT uniquement
- ‚úÖ [5.4] Initial Generation
- ‚ö†Ô∏è [5.5] Self-RAG ‚Üí Si ambiguous uniquement
- ‚ö†Ô∏è [5.6] Grounded Generation ‚Üí Document-level citations
- ‚úÖ [5.7] Hallucination Detection ‚Üí LettuceDetect + TLM
- ‚úÖ [5.8] Multi-Stage Validation ‚Üí Faithfulness + Attribution + Completeness
- ‚ùå [5.9] Refinement ‚Üí D√©sactiv√© (trop lent)
- ‚úÖ [5.10] Post-Processing ‚Üí Structured

**Gains** :
- Latence : **3.8s** (+52%)
- Qualit√© : **+15%** (65% ‚Üí 75%)
- Faithfulness : **+10%** (0.78 ‚Üí 0.86)
- Hallucinations : **-40%** (18% ‚Üí 11%)
- Co√ªt : **+25%**

---

#### Preset : **maximal** (Qualit√© maximale)

**Objectif** : Qualit√© et fiabilit√© maximales, queries complexes.

**Features activ√©es** :
- ‚úÖ [5.1] Pre-Generation Analysis ‚Üí Full + CRAG
- ‚úÖ [5.2] Adaptive Prompting ‚Üí Full
- ‚úÖ [5.3] Advanced Techniques ‚Üí CoT + Self-Consistency + Few-Shot
- ‚úÖ [5.4] Initial Generation ‚Üí Structured Output
- ‚úÖ [5.5] Self-RAG ‚Üí Full (retrieve on-demand)
- ‚úÖ [5.6] GINGER ‚Üí Claim-level citations
- ‚úÖ [5.7] Hallucination Detection ‚Üí TLM + LLM-as-Judge
- ‚úÖ [5.8] Multi-Stage Validation ‚Üí Full (5 stages)
- ‚úÖ [5.9] Response Refinement ‚Üí 1-2 iterations
- ‚úÖ [5.10] Post-Processing ‚Üí Rich formatting

**Gains** :
- Latence : **6-8s** (+140-220%)
- Qualit√© : **+30%** (65% ‚Üí 85%)
- Faithfulness : **+18%** (0.78 ‚Üí 0.92)
- Hallucinations : **-61%** (18% ‚Üí 7%)
- Attribution : **+45%** (55% ‚Üí 80%)
- Co√ªt : **+80%**

---

#### Preset : **cost_optimized** (Co√ªts minimaux)

**Objectif** : Minimiser co√ªts API, maximiser caching.

**Features activ√©es** :
- ‚úÖ [5.1] Pre-Generation Analysis ‚Üí Heuristic (pas LLM)
- ‚ö†Ô∏è [5.2] Static Prompting ‚Üí Templates fixes
- ‚ùå [5.3] Advanced Techniques ‚Üí D√©sactiv√©
- ‚úÖ [5.4] Initial Generation ‚Üí Structured Output (pas de retry)
- ‚ùå [5.5] Self-RAG ‚Üí D√©sactiv√© (re-retrieval co√ªteux)
- ‚ùå [5.6] Grounded Generation ‚Üí Citations basiques
- ‚ö†Ô∏è [5.7] Hallucination Detection ‚Üí Local model uniquement (LettuceDetect)
- ‚ö†Ô∏è [5.8] Validation ‚Üí Faithfulness uniquement
- ‚ùå [5.9] Refinement ‚Üí D√©sactiv√©
- ‚úÖ [5.10] Post-Processing ‚Üí Simple
- ‚úÖ **Aggressive Caching** : TTL 24h, query similarity

**Gains** :
- Latence : **2.8s** (+12%)
- Qualit√© : **+8%** (65% ‚Üí 70%)
- Co√ªt : **-40%** (caching + structured output + no refinement)

---

#### Preset : **high_assurance** (M√©dical, L√©gal, Critique)

**Objectif** : Fiabilit√© maximale, zero tolerance hallucinations.

**Features activ√©es** :
- ‚úÖ [5.1] Pre-Generation Analysis ‚Üí Full + CRAG strict
- ‚úÖ [5.2] Adaptive Prompting ‚Üí Conservative
- ‚úÖ [5.3] Extractive Answering ‚Üí Pr√©f√©rer extraction sur g√©n√©ration
- ‚úÖ [5.4] Initial Generation ‚Üí Structured + low temperature (0.0)
- ‚úÖ [5.5] Self-RAG ‚Üí Full avec thresholds stricts
- ‚úÖ [5.6] GINGER ‚Üí Claim-level + source verification
- ‚úÖ [5.7] Hallucination Detection ‚Üí TLM + LLM-as-Judge + Human-in-loop
- ‚úÖ [5.8] Multi-Stage Validation ‚Üí Full + strict thresholds
- ‚ö†Ô∏è [5.9] Refinement ‚Üí Si √©chec validation uniquement
- ‚úÖ [5.10] Post-Processing ‚Üí Audit trail + confidence scores
- ‚úÖ **Refuse to Answer** : Si moindre doute

**Gains** :
- Latence : **5-7s** (+100-180%)
- Qualit√© : **+25%** (65% ‚Üí 81%)
- Hallucinations : **-72%** (18% ‚Üí 5%)
- Attribution : **+50%** (55% ‚Üí 83%)
- Refusal Rate : **+200%** (mieux refuser que halluciner)
- Co√ªt : **+70%**

---

## 7. RECOMMANDATIONS

### üéØ Recommandations par Phase

#### Phase 1 : D√©marrage (MVP)

**Priorit√© : Fonctionnel + Fast**

1. ‚úÖ Impl√©menter **preset "minimal"**
2. ‚úÖ Activer **LettuceDetect** (hallucination detection l√©ger)
3. ‚úÖ Activer **Faithfulness validation** (RAGAS)
4. ‚úÖ Citations document-level ([1], [2])
5. ‚úÖ Structured Output (JSON Schema si API support√©)

**Gains attendus** : +5% qualit√©, latence baseline

---

#### Phase 2 : Production Standard (Recommand√©)

**Priorit√© : √âquilibre qualit√©/co√ªt**

1. ‚úÖ D√©ployer **preset "balanced"** ‚≠ê
2. ‚úÖ Activer **CRAG** (retrieval evaluator)
3. ‚úÖ Activer **Adaptive RAG** (query complexity routing)
4. ‚úÖ Hallucination Detection : **LettuceDetect + TLM**
5. ‚úÖ Multi-Stage Validation (Faithfulness + Attribution + Completeness)
6. ‚ö†Ô∏è Self-RAG : **conditionnel** (si query ambig√ºe)

**Gains attendus** : +15% qualit√©, +52% latence, +25% co√ªt

---

#### Phase 3 : Excellence (High-Quality)

**Priorit√© : Qualit√© maximale**

1. ‚úÖ D√©ployer **preset "maximal"**
2. ‚úÖ Activer **Self-RAG** (full)
3. ‚úÖ Activer **GINGER** (claim-level citations)
4. ‚úÖ Activer **DSPy** (prompt optimization)
5. ‚úÖ Hallucination Detection : **TLM + LLM-as-Judge**
6. ‚úÖ Response Refinement (1-2 iterations)
7. ‚úÖ Structured Output + Self-Consistency

**Gains attendus** : +30% qualit√©, +140% latence, +80% co√ªt

---

### üîß Configuration Technique Recommand√©e

#### LLM Choice (2025)

| Use Case | Recommandation | Rationale |
|----------|----------------|-----------|
| **Free/Local** | Llama 3.1 70B (Ollama) | Excellent qualit√©, gratuit, 128k context |
| **Cost-Optimized** | GPT-4o-mini | $0.15/1M input, rapide, structured output natif |
| **Best Quality** | Claude 3.5 Sonnet | 200k context, excellent faithfulness |
| **Long Context** | Gemini 1.5 Pro | 1M context, bon rapport qualit√©/prix |

#### Temperature Settings

```yaml
# Recommandations 2025
factual_queries: 0.0-0.1   # D√©terministe, pas de cr√©ativit√©
analytical_queries: 0.1-0.2 # L√©g√®rement cr√©atif pour analyse
creative_queries: 0.5-0.7   # Cr√©atif (hors scope RAG)
```

#### Context Window Management

```yaml
# Budget tokens (mod√®le 128k)
system_prompt: 500 tokens
context_compressed: 4000-6000 tokens  # Phase 04
query: 50-200 tokens
completion: 1500-2000 tokens
buffer: 1000 tokens
---
Total: ~7-10k tokens utilis√©s / 128k disponibles
```

---

### üìä M√©triques √† Tracker

#### M√©triques de Qualit√©
- **Answer Quality Score** : 0-1 (√©val humaine ou LLM-as-Judge)
- **Faithfulness** : RAGAS faithfulness score
- **Hallucination Rate** : % r√©ponses avec hallucinations d√©tect√©es
- **Attribution Accuracy** : % citations correctes
- **Citation Recall** : % sources cit√©es / sources utilis√©es
- **Completeness** : La r√©ponse couvre-t-elle toute la question ?

#### M√©triques de Performance
- **Latency P50/P95/P99** : Latence g√©n√©ration
- **Tokens Prompt** : Tokens input
- **Tokens Completion** : Tokens output
- **Cost per Query** : Co√ªt moyen par requ√™te

#### M√©triques Business
- **User Satisfaction** : Score utilisateur (1-10)
- **Trust Score** : L'utilisateur fait-il confiance √† la r√©ponse ?
- **Refusal Rate** : % "insufficient context"
- **Follow-up Questions** : % queries avec follow-up (indicateur ambigu√Øt√©)

---

### üöÄ Roadmap d'Impl√©mentation

#### Semaine 1-2 : Foundation
- [ ] Impl√©menter preset "minimal"
- [ ] Configurer LettuceDetect (hallucination detection)
- [ ] Ajouter Faithfulness validation (RAGAS)
- [ ] Tester baseline performance

#### Semaine 3-4 : Quality Boost
- [ ] Migrer vers preset "balanced"
- [ ] Impl√©menter CRAG (retrieval evaluator)
- [ ] Ajouter TLM (deep hallucination check)
- [ ] Multi-Stage Validation

#### Semaine 5-6 : Advanced Features
- [ ] Impl√©menter Adaptive RAG (query routing)
- [ ] Self-RAG conditionnel (queries ambig√ºes)
- [ ] Structured Output (JSON Schema)
- [ ] A/B Testing balanced vs minimal

#### Semaine 7-8 : Optimization
- [ ] DSPy integration (prompt optimization)
- [ ] GINGER (claim-level citations)
- [ ] Response Refinement (si KPI critiques)
- [ ] Monitoring dashboard

---

### ‚ö†Ô∏è Trade-offs Critiques

| Decision | Gain | Co√ªt | Quand Choisir ? |
|----------|------|------|-----------------|
| **Self-RAG ON** | +12% qualit√© | +1s latence | Queries complexes/ambig√ºes uniquement |
| **Response Refinement ON** | +8% qualit√© | +2s latence | High-stakes uniquement (m√©dical, l√©gal) |
| **GINGER Claims** | +25% attribution | +200ms | Si tra√ßabilit√© critique |
| **LLM-as-Judge** | +10% detection | +500ms | Fallback critique uniquement |
| **DSPy Optimization** | +8% qualit√© | 3h compile | Si dataset train/eval disponible |

---

## üìö R√âF√âRENCES

### Papers Cl√©s (2025)

1. **Self-RAG** : Asai et al., 2024 - "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection"
2. **CRAG** : Yan et al., 2024 - "Corrective Retrieval Augmented Generation"
3. **Adaptive RAG** : Jeong et al., 2024 - "Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity"
4. **GINGER** : Li et al., SIGIR 2025 - "Grounded Information Nugget-Based Generation of Responses"
5. **LettuceDetect** : 2025 - "Hallucination Detection Framework for RAG Applications"
6. **TLM** : Cleanlab, 2025 - "Trustworthy Language Model"
7. **DSPy** : Khattab et al., Stanford NLP - "DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines"

### Benchmarks
- **RAGTruth** : 18k responses corpus for hallucination analysis
- **JSONSchemaBench** : 10k schemas for structured output evaluation
- **RAGAS** : RAG evaluation suite (faithfulness, answer_relevancy, context_recall)

### Tools & Libraries
- **DSPy** : `pip install dspy-ai`
- **LLMLingua** : `pip install llmlingua` (Phase 04)
- **RAGAS** : `pip install ragas`
- **Guidance** : `pip install guidance`
- **Outlines** : `pip install outlines`
- **LettuceDetect** : HuggingFace `adaamko/lettucedetect`
- **Cleanlab TLM** : `pip install cleanlab`

---

## üéØ CONCLUSION

La Phase 05 v2 introduit **10 sous-√©tapes avanc√©es** vs 7 sections basiques en v1, avec des gains de qualit√© de **+15% √† +30%** selon le preset choisi.

**Recommandation prioritaire** :
1. ‚úÖ **D√©marrer avec preset "balanced"** (meilleur ROI)
2. ‚úÖ **Activer hallucination detection** (LettuceDetect + TLM)
3. ‚úÖ **CRAG + Adaptive RAG** (qualit√© + efficience)
4. ‚ö†Ô∏è **Self-RAG conditionnel** (queries complexes uniquement)
5. ‚ùå **Reporter Response Refinement** (trop lent, gains marginaux)

**"L'√®re du RAG statique est termin√©e. Les syst√®mes adaptatifs, auto-correctifs et multimodaux sont d√©sormais mainstream."**

---

**Prochaines √©tapes** :
- Cr√©er `05_generation_v2.yaml` (configuration d√©taill√©e balanced)
- Cr√©er `05_generation_v2_modular.yaml` (avec presets et flags granulaires)
