# ğŸ§  inference â€“ Serveur dâ€™infÃ©rence ML multi-providers Python performant

> Pipeline RAG ultime 2025 : Query Expansion, Retrieval Hybride Triple, Reranking Multi-Ã‰tages, Compression Contextuelle, GÃ©nÃ©ration, Ã‰valuation.

## ğŸš€ Objectif

Le projet **inference** est un serveur d'infÃ©rence haute performance conÃ§u pour orchestrer des pipelines de **Retrieval-Augmented Generation (RAG)** sophistiquÃ©s. Il permet de passer du prototypage Ã  la production en intÃ©grant les meilleures pratiques de l'Ã©tat de l'art (SOTA 2025).

Points forts :
- **Abstraction unifiÃ©e** pour OpenAI, Anthropic, Ollama et modÃ¨les locaux.
- **Pipeline modulaire** en 5 phases : Embedding, Retrieval, Reranking, Compression, Generation.
- **ConformitÃ© stricte** : PEP 621, PEP 484 (Mypy strict), PEP 257 (Google style), PEP 8 (Ruff).

---

## ğŸ—ï¸ Architecture (5 phases)

`Query â†’ [01] Embedding â†’ [02] Retrieval â†’ [03] Reranking â†’ [04] Compression â†’ [05] Generation â†’ Answer`

### **Phase 01 - Traitement des requÃªtes** âœ…
*   DÃ©composition multi-sauts automatique (`QueryDecomposer`).
*   Routage adaptatif (simple/standard/complexe) via `QueryRouter`.
*   Expansion de requÃªte (HyDE, CoT, Multi-query) avec `QueryExpansionModule`.

### **Phase 02 - RÃ©cupÃ©ration hybride** âœ…
*   RÃ©cupÃ©ration itÃ©rative multi-sauts (jusqu'Ã  3 sauts) via `IterativeRetriever`.
*   Filtrage intelligent des mÃ©tadonnÃ©es (`MetadataFilter`).
*   Approche **Triple Hybride** : Dense (BGE-M3/OpenAI) + Sparse (BM25) + Fusion RRF.

### **Phase 03 - RÃ©organisation (Reranking)** âœ…
*   LLM Reranking style RankGPT (`LLMReranker`).
*   Cross-Encoder haute prÃ©cision (BGE-Reranker-v2-M3).
*   RÃ©Ã©valuation de la diversitÃ© avec l'algorithme MMR.

### **Phase 04 - Compression contextuelle** âœ…
*   Compression extractive intelligente via `LLMLingua`.
*   Optimisation drastique de la fenÃªtre de contexte (-47% tokens en moyenne).

### **Phase 05 - GÃ©nÃ©ration avancÃ©e** âœ…
*   Raffinement itÃ©ratif des rÃ©ponses avec autocorrection.
*   Sortie structurÃ©e (JSON Schema) garantie.
*   DÃ©tection native des hallucinations (NLI).

---

## âš™ï¸ Stack technique

- **Langage** : Python 3.9+ (Pin 3.12 recommandÃ©)
- **Frameworks** : FastAPI, LangChain, DSPy
- **Embeddings & LLM** : OpenAI, Anthropic, Hugging Face (Sentence-Transformers)
- **Vector Stores** : ChromaDB, Qdrant, Faiss
- **QualitÃ©** : Ruff (format & lint), Mypy (strict), Pytest (95%+ couverture)

---

## ğŸ“¦ Installation

Le projet utilise `rye` ou `pip` standard.

```bash
# 1. Cloner le dÃ©pÃ´t
git clone https://github.com/dagornc/inference.git
cd inference

# 2. Installer les dÃ©pendances
pip install -r requirements.txt
# Ou via rye
rye sync --all-features
```

---

## â–¶ï¸ Utilisation rapide

```python
from inference_project.steps import (
    EmbeddingStep, 
    RetrievalStep, 
    RerankingStep, 
    GenerationStep
)

# Initialisation des Ã©tapes
query = \"Explique-moi le fonctionnement d'un pipeline RAG hybride.\"
emb_step = EmbeddingStep()
ret_step = RetrievalStep()

# ExÃ©cution du pipeline
emb_result = emb_step.execute(query)
ret_result = ret_step.execute(
    query_embeddings=emb_result[\"embeddings\"],
    sub_queries=emb_result.get(\"sub_queries\", [query])
)

print(f\"Documents trouvÃ©s : {len(ret_result['documents'])}\")
```

---

## ğŸ§ª Tests & QualitÃ©

```bash
# Formater et vÃ©rifier le code
ruff format src/
ruff check src/ --fix

# Lancer les tests avec couverture
pytest tests/ --cov=src/inference_project
```

---

## ğŸ—ºï¸ Roadmap

- [ ] Support des modÃ¨les d'interaction tardive (ColBERT/RAGatouille).
- [ ] Couche de cache distribuÃ©e avec Redis.
- [ ] Optimisation automatique des prompts via DSPy.
- [ ] Support des embeddings clairsemÃ©s SPLADE.

---

## ğŸ“„ Licence

Ce projet est sous licence **MIT**.
