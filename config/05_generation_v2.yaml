# =============================================================================
# √âTAPE 05 v2 : G√âN√âRATION & PROMPT ENGINEERING - CONFIGURATION COMPL√àTE
# =============================================================================
# Configuration d√©taill√©e et enrichie de l'√©tape de g√©n√©ration avec techniques
# avanc√©es 2025 : Self-RAG, CRAG, Adaptive RAG, hallucination detection.
#
# NOUVEAUT√âS v2 :
# - [5.1] Pre-Generation Analysis : CRAG evaluator + query complexity
# - [5.2] Prompt Construction Adaptive : Prompts selon query type
# - [5.3] Advanced Prompting : CoT, Self-Consistency, Few-Shot
# - [5.4] Initial Generation : Structured output, streaming
# - [5.5] Self-RAG : Retrieve on-demand + self-reflection tokens
# - [5.6] Grounded Generation : GINGER, claim-level citations
# - [5.7] Hallucination Detection : LettuceDetect, TLM, LLM-as-Judge
# - [5.8] Multi-Stage Validation : Faithfulness + Attribution + Consistency
# - [5.9] Response Refinement : Iterative self-correction
# - [5.10] Post-Processing : Rich formatting, audit trail
#
# GAINS ATTENDUS v2 (preset "balanced") :
# - Qualit√© : +15% (65% ‚Üí 75%)
# - Faithfulness : +10% (0.78 ‚Üí 0.86)
# - Hallucinations : -40% (18% ‚Üí 11%)
# - Latence : +52% (2.5s ‚Üí 3.8s)
# - Co√ªt : +25%
# =============================================================================

# =============================================================================
# üìã CONFIGURATION PRINCIPALE
# =============================================================================

step_05_generation:
  enabled: true
  version: "v2"

  # Configuration "balanced" activ√©e par d√©faut
  # √âquilibre qualit√©/co√ªt/latence optimal
  description: "G√©n√©ration intelligente avec Self-RAG, CRAG et validation multi-niveaux"

  # M√©triques cibles (balanced)
  targets:
    answer_quality: 0.75           # +15% vs baseline (0.65)
    faithfulness_score: 0.86       # +10% vs baseline (0.78)
    hallucination_rate: 0.11       # -40% vs baseline (0.18)
    latency_budget_ms: 3800        # +52% vs baseline (2500ms)
    cost_multiplier: 1.25          # +25% co√ªt

  # Pipeline d'ex√©cution s√©quentiel
  pipeline:
    - step: "pre_generation_analysis"
      enabled: true
      latency_budget_ms: 200

    - step: "prompt_construction"
      enabled: true
      latency_budget_ms: 50

    - step: "advanced_prompting"
      enabled: true
      latency_budget_ms: 0  # Inline dans prompt

    - step: "initial_generation"
      enabled: true
      latency_budget_ms: 2000

    - step: "self_rag"
      enabled: true
      conditional: true  # Si query ambig√ºe uniquement
      latency_budget_ms: 1000

    - step: "grounded_generation"
      enabled: false  # D√©sactiv√© (balanced = citations document-level)
      latency_budget_ms: 0

    - step: "hallucination_detection"
      enabled: true
      latency_budget_ms: 200

    - step: "multi_stage_validation"
      enabled: true
      latency_budget_ms: 250

    - step: "response_refinement"
      enabled: false  # D√©sactiv√© (trop lent)
      latency_budget_ms: 0

    - step: "post_processing"
      enabled: true
      latency_budget_ms: 100

# =============================================================================
# [5.1] PRE-GENERATION ANALYSIS
# =============================================================================

pre_generation_analysis:
  enabled: true
  description: "Analyse de complexit√© query + √©valuation qualit√© contexte (CRAG)"

  # Analyse de complexit√© de la query
  query_complexity:
    enabled: true
    method: "hybrid"  # hybrid/llm/heuristic

    # Heuristic-based (rapide, 0ms)
    heuristic_config:
      factors:
        - query_length         # Longueur query
        - num_entities         # Nombre entit√©s
        - question_words       # Mots interrogatifs (who/what/why/how)
        - comparison_words     # Mots comparatifs (vs/compare/better)
        - temporal_words       # Mots temporels (when/since/until)

      scoring:
        simple: 0.3      # Score < 0.3
        medium: 0.6      # Score 0.3-0.6
        complex: 1.0     # Score > 0.6

    # LLM-based (plus pr√©cis, +100ms)
    llm_config:
      enabled: false  # D√©sactiv√© (balanced = heuristic)
      provider: "ollama"
      model: "llama3"
      prompt_template: |
        Classify the complexity of this query: "{query}"

        Complexity levels:
        - SIMPLE: Factual, single-hop, one entity
        - MEDIUM: Multi-entity, requires reasoning
        - COMPLEX: Multi-hop, analytical, comparative

        Answer with one word: SIMPLE, MEDIUM, or COMPLEX.

  # CRAG : √âvaluation qualit√© du contexte r√©cup√©r√©
  crag_evaluator:
    enabled: true
    description: "Corrective RAG - √âvalue qualit√© docs avant g√©n√©ration"

    # M√©thode d'√©valuation
    method: "lightweight"  # lightweight/cross_encoder/llm

    # Lightweight evaluator (rapide, +50ms)
    lightweight_config:
      model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
      batch_size: 32

      # Thresholds de d√©cision
      thresholds:
        correct: 0.7        # Score > 0.7 : OK, g√©n√©rer
        ambiguous: 0.4      # Score 0.4-0.7 : Docs partiels
        incorrect: 0.4      # Score < 0.4 : Mauvais docs

    # Actions correctives
    actions:
      correct:
        action: "generate"
        description: "Documents suffisants, g√©n√©rer directement"

      ambiguous:
        action: "decompose"
        description: "Documents partiels, d√©composer en knowledge strips"
        knowledge_strips:
          enabled: true
          segment_by: "relevance"  # relevance/entity/topic
          min_relevance: 0.5

      incorrect:
        action: "web_search"
        description: "Documents insuffisants, fallback web search"
        web_search:
          enabled: false  # D√©sactiv√© par d√©faut (Phase 02 d√©j√† web search)
          provider: "duckduckgo"
          num_results: 5

  # S√©lection de strat√©gie adaptative
  strategy_selection:
    enabled: true
    description: "Adaptive RAG - Strat√©gie selon complexit√© query"

    strategies:
      simple:
        name: "direct_generation"
        retrieval_depth: 1
        reranking: false
        chain_of_thought: false
        self_correction: false
        target_latency_ms: 2000

      medium:
        name: "standard_rag"
        retrieval_depth: 1
        reranking: true
        chain_of_thought: false
        self_correction: false
        target_latency_ms: 3500

      complex:
        name: "multi_hop_cot"
        retrieval_depth: 2
        reranking: true
        chain_of_thought: true
        self_correction: true
        target_latency_ms: 6000

# =============================================================================
# [5.2] PROMPT CONSTRUCTION (ADAPTIVE)
# =============================================================================

prompt_construction:
  enabled: true
  description: "Construction adaptative du prompt selon query type"

  # S√©lection du system prompt selon query type
  system_prompt_adaptive:
    enabled: true

    prompts:
      factual: |
        Tu es un assistant expert qui r√©pond aux questions factuelles en te basant
        UNIQUEMENT sur le contexte fourni. Tes r√©ponses doivent √™tre:
        - Pr√©cises et concises
        - Bas√©es exclusivement sur le contexte
        - Accompagn√©es de citations [1], [2]

        Si le contexte ne contient pas la r√©ponse, dis: "Je n'ai pas trouv√©
        suffisamment d'informations dans le contexte fourni."

      analytical: |
        Tu es un assistant expert en analyse qui r√©pond aux questions analytiques
        en te basant sur le contexte fourni. Tes r√©ponses doivent:
        - Analyser les diff√©rentes perspectives
        - Raisonner √©tape par √©tape
        - Citer les sources [1], [2]
        - Identifier les limites de l'analyse

        Si le contexte est insuffisant, indique clairement les informations manquantes.

      comparative: |
        Tu es un assistant expert en comparaison qui analyse et compare les √©l√©ments
        en te basant sur le contexte fourni. Tes r√©ponses doivent:
        - Identifier les similitudes et diff√©rences
        - Pr√©senter de mani√®re structur√©e (tableau ou liste)
        - Citer les sources pour chaque point [1], [2]
        - Rester neutre et factuel

        Si le contexte ne permet pas la comparaison, indique les informations manquantes.

    fallback: |
      Tu es un assistant expert qui r√©pond aux questions en te basant UNIQUEMENT
      sur le contexte fourni ci-dessous. Tes r√©ponses doivent √™tre pr√©cises,
      factuelles, et accompagn√©es de citations des sources [1], [2], etc.

      Si le contexte ne contient pas assez d'informations, tu DOIS dire:
      "Je n'ai pas trouv√© suffisamment d'informations dans le contexte fourni."

  # Format du contexte
  context_formatting:
    # Template pour chaque document
    document_template: |
      [{doc_id}] (Source: {source}, Score: {score:.3f})
      {content}

    # S√©parateur entre documents
    separator: "\n\n---\n\n"

    # Options de formatage
    include_metadata: true       # Inclure source, score
    sort_by_relevance: true      # Trier par score d√©croissant
    number_documents: true       # Num√©roter [1], [2], etc.
    truncate_per_doc: 500        # Tokens max par doc

  # Template du prompt utilisateur
  user_prompt_template: |
    Contexte ({num_docs} documents) :
    {context}

    Question : {query}

    Instructions :
    - R√©ponds √† la question en utilisant UNIQUEMENT les informations du contexte
    - Cite tes sources en utilisant les num√©ros [1], [2], etc.
    - Si la r√©ponse n'est pas dans le contexte, dis-le clairement
    - Sois concis mais complet

    R√©ponse :

# =============================================================================
# [5.3] ADVANCED PROMPTING TECHNIQUES
# =============================================================================

advanced_prompting:
  enabled: true
  description: "Techniques avanc√©es de prompt engineering"

  # Chain-of-Thought (CoT)
  chain_of_thought:
    enabled: true
    conditional: true  # Uniquement si query complex
    trigger_on_complexity: ["complex"]

    prompt_addition: |

      Avant de r√©pondre, analyse d'abord:
      1. Quels sont les faits cl√©s du contexte ?
      2. Comment ces faits se connectent-ils √† la question ?
      3. Quel raisonnement √©tape par √©tape m√®ne √† la r√©ponse ?

      Puis fournis la r√©ponse finale.

    format: "explicit"  # explicit/implicit
    # explicit : demander explicitement les √©tapes
    # implicit : "Let's think step by step"

  # Few-Shot Prompting
  few_shot:
    enabled: false  # D√©sactiv√© (balanced = zero-shot)
    description: "Exemples de Q&A pour guider le format"

    # Source des exemples
    source: "static"  # static/dynamic/dspy

    # Exemples statiques
    static_examples:
      - query: "Quelle est la politique de mot de passe ?"
        context: |
          [1] La politique de s√©curit√© exige des mots de passe d'au moins
          12 caract√®res avec chiffres, lettres et symboles. Expiration tous les 90 jours.
        answer: |
          La politique de mot de passe exige au moins 12 caract√®res incluant
          chiffres, lettres et symboles [1]. Les mots de passe expirent tous
          les 90 jours [1].

      - query: "Quel est le d√©lai de livraison ?"
        context: |
          [1] Les commandes sont exp√©di√©es sous 24-48h et livr√©es sous 3-5 jours
          ouvr√©s en France m√©tropolitaine.
        answer: |
          Les commandes sont livr√©es sous 3-5 jours ouvr√©s en France
          m√©tropolitaine [1], avec exp√©dition sous 24-48h [1].

    # Nombre d'exemples √† inclure
    num_examples: 2

  # Self-Consistency (Multiple Sampling)
  self_consistency:
    enabled: false  # D√©sactiv√© (balanced = single generation)
    description: "G√©n√©rer plusieurs r√©ponses et prendre la plus coh√©rente"

    num_samples: 3             # Nombre de g√©n√©rations
    temperature: 0.5           # Temp√©rature pour diversit√©
    aggregation: "voting"      # voting/clustering/llm_judge

    voting_config:
      similarity_threshold: 0.85  # Similarit√© min pour agr√©gation
      min_agreement: 2            # Nombre min samples identiques

  # Extractive Answering
  extractive:
    enabled: false
    description: "Extraction exacte du texte sans reformulation"

    prompt_override: |
      Extrais le passage le plus pertinent du contexte qui r√©pond √† la question.
      Retourne UNIQUEMENT le texte exact sans modification ni reformulation.
      Cite la source [num√©ro].

  # Contrastive Prompting
  contrastive:
    enabled: false
    conditional: true
    trigger_on_query_type: ["comparative", "analytical"]

    prompt_override: |
      Fournis une analyse √©quilibr√©e de la question bas√©e sur le contexte :

      Avantages / Arguments pour :
      - [Point 1 avec citation]
      - [Point 2 avec citation]

      Inconv√©nients / Arguments contre :
      - [Point 1 avec citation]
      - [Point 2 avec citation]

      Conclusion bas√©e sur le contexte :
      - [Synth√®se factuelle]

# =============================================================================
# [5.4] INITIAL GENERATION
# =============================================================================

initial_generation:
  enabled: true
  description: "G√©n√©ration initiale de la r√©ponse"

  # Configuration du LLM
  llm:
    # Provider (d√©fini dans global.yaml)
    provider: "ollama"  # ollama/openai/anthropic/mistral_ai

    # Mod√®le
    model: "llama3"  # llama3/gpt-4o-mini/claude-3-5-sonnet-20241022

    # Param√®tres de g√©n√©ration
    temperature: 0.0      # D√©terministe pour RAG
    max_tokens: 1000      # Longueur max r√©ponse
    top_p: 0.95
    presence_penalty: 0.0
    frequency_penalty: 0.0

    # Stop sequences
    stop_sequences: []

    # Timeout
    timeout: 60

  # Structured Output (JSON Schema)
  structured_output:
    enabled: false  # D√©sactiv√© (balanced = markdown text)
    description: "Constrained decoding avec JSON Schema"

    # Framework
    framework: "guidance"  # guidance/outlines/xgrammar/openai

    # Schema Pydantic
    schema_definition: |
      from pydantic import BaseModel

      class Citation(BaseModel):
          doc_id: int
          quote: str

      class RAGResponse(BaseModel):
          answer: str
          confidence: float
          citations: list[Citation]
          is_sufficient_context: bool
          reasoning: str  # Si CoT activ√©

    # Guidance config
    guidance_config:
      model: "llama3"
      device: "cpu"

  # Streaming
  streaming:
    enabled: false  # D√©sactiv√© (validation post-g√©n√©ration n√©cessaire)
    description: "Affichage progressif de la r√©ponse"

# =============================================================================
# [5.5] SELF-RAG (ADAPTIVE RETRIEVAL & REFLECTION)
# =============================================================================

self_rag:
  enabled: true
  conditional: true  # Uniquement si query ambig√ºe ou CRAG = ambiguous
  description: "Self-RAG avec retrieve on-demand et self-reflection"

  # Triggers d'activation
  activation_triggers:
    query_complexity: ["complex"]      # Si query complexe
    crag_score: ["ambiguous"]          # Si CRAG = ambiguous
    initial_confidence_low: true       # Si confidence < threshold

  # Tokens sp√©ciaux Self-RAG
  special_tokens:
    retrieve_token: "[Retrieve?]"      # D√©cision retrieve
    is_relevant_token: "[IsRelevant]"  # √âvaluation relevance docs
    is_supported_token: "[IsSupported]" # Support g√©n√©ration
    is_useful_token: "[IsUseful]"      # Utilit√© g√©n√©ration

  # Configuration retrieve on-demand
  retrieve_on_demand:
    enabled: true
    max_iterations: 2          # Max re-retrievals

    # D√©cision de retrieve
    retrieve_decision:
      method: "llm"  # llm/heuristic
      llm_prompt: |
        Given the question: "{query}"
        And the current partial answer: "{partial_answer}"

        Do you need to retrieve more information?
        Answer: YES or NO

  # Reflection tokens
  reflection:
    enabled: true

    # √âvaluation relevance des documents
    is_relevant:
      enabled: true
      scale: [1, 3, 5]  # 1=Not relevant, 3=Partially, 5=Highly relevant
      threshold: 3      # Minimum pour accepter docs

    # √âvaluation support de la g√©n√©ration
    is_supported:
      enabled: true
      options: ["Fully", "Partially", "No"]
      threshold: "Partially"  # Minimum acceptable

    # √âvaluation utilit√© de la g√©n√©ration
    is_useful:
      enabled: true
      scale: [1, 3, 5]  # 1=Not useful, 3=Acceptable, 5=Highly useful
      threshold: 3

  # Re-generation si √©chec reflection
  regenerate_on_failure:
    enabled: true
    max_attempts: 2

# =============================================================================
# [5.6] GROUNDED GENERATION & ATTRIBUTION (GINGER)
# =============================================================================

grounded_generation:
  enabled: false  # D√©sactiv√© (balanced = document-level citations)
  description: "Attribution granulaire avec information nuggets (GINGER)"

  # Extraction d'information nuggets
  nugget_extraction:
    enabled: false
    method: "llm"  # llm/rule_based

    llm_config:
      provider: "ollama"
      model: "llama3"
      prompt_template: |
        Extract atomic facts (information nuggets) from this document:

        Document: {document}

        Output format:
        - Fact 1: [atomic fact]
        - Fact 2: [atomic fact]
        ...

  # Clustering de nuggets
  nugget_clustering:
    enabled: false
    algorithm: "kmeans"  # kmeans/hierarchical
    num_clusters: 5

  # Ranking de nuggets
  nugget_ranking:
    enabled: false
    scorer_model: "BAAI/bge-m3"
    top_k: 15

  # G√©n√©ration √† partir de nuggets
  nugget_generation:
    enabled: false
    method: "summarization"
    preserve_nugget_mapping: true  # Tracer nugget ‚Üí phrase g√©n√©r√©e

  # Attribution claim-level
  claim_attribution:
    enabled: false  # En v2 = document-level, future v3 = claim-level
    method: "nli"   # Natural Language Inference
    nli_model: "microsoft/deberta-v3-large-nli"
    attribution_threshold: 0.85

# =============================================================================
# [5.7] HALLUCINATION DETECTION
# =============================================================================

hallucination_detection:
  enabled: true
  description: "D√©tection multi-niveaux des hallucinations"

  # Cascade de d√©tection : lightweight ‚Üí deep ‚Üí llm-judge
  cascade:
    enabled: true
    stop_on_first_detection: false  # Continuer cascade m√™me si d√©tection

  # Method 1 : LettuceDetect (l√©ger, rapide)
  lettucedetect:
    enabled: true
    model: "adaamko/lettucedetect"
    device: "cpu"
    threshold: 0.5          # Probabilit√© hallucination
    action: "flag"          # flag/reject/continue_cascade

  # Method 2 : TLM (Trustworthy Language Model)
  tlm:
    enabled: true
    conditional: true       # Si LettuceDetect doute (score 0.3-0.7)
    api_key: "${CLEANLAB_API_KEY}"
    threshold: 0.7          # Trustworthiness score min
    action: "flag"

  # Method 3 : LLM-as-a-Judge
  llm_judge:
    enabled: false  # D√©sactiv√© (balanced = LettuceDetect + TLM suffisants)
    conditional: true
    trigger_on: "critical_queries"  # Uniquement queries critiques

    provider: "openai"
    model: "gpt-4o-mini"
    prompt_template: |
      Context: {context}

      Question: {query}

      Generated Answer: {answer}

      Does the answer contain any hallucinations (information not in the context)?
      Respond with:
      - "NO" if fully grounded
      - "MINOR" if minor unsupported claims
      - "MAJOR" if major hallucinations

      Answer:

  # Method 4 : Self-Evaluation (inline)
  self_evaluation:
    enabled: true
    description: "Le LLM s'auto-√©value pendant g√©n√©ration (Self-RAG tokens)"
    # Utilise les reflection tokens de Self-RAG [IsSupported]

  # Method 5 : RAGAS Faithfulness
  ragas_faithfulness:
    enabled: true
    min_score: 0.85
    action: "warn"  # warn/reject

  # Actions selon d√©tection
  actions:
    on_hallucination_detected:
      action: "flag_and_return"  # flag_and_return/reject/regenerate
      flag_format: |
        ‚ö†Ô∏è Warning: Potential hallucination detected (confidence: {confidence})
        {answer}

# =============================================================================
# [5.8] MULTI-STAGE VALIDATION
# =============================================================================

multi_stage_validation:
  enabled: true
  description: "Validation qualit√© multi-niveaux"

  # Stage 1 : Faithfulness (Fid√©lit√© au contexte)
  faithfulness:
    enabled: true
    method: "ragas"  # ragas/nli/semantic_similarity

    ragas_config:
      min_score: 0.85
      action: "warn"  # warn/reject/regenerate

    nli_config:
      model: "microsoft/deberta-v3-large-nli"
      threshold: 0.85

    action_on_failure: "warn"

  # Stage 2 : Attribution (Citations valides)
  attribution:
    enabled: true

    checks:
      citations_exist: true          # Au moins 1 citation
      citations_accurate: true       # Citations pointent vers docs existants
      min_citations: 1
      max_citations_per_claim: 3

    action_on_failure: "warn"

  # Stage 3 : Consistency (Coh√©rence interne)
  consistency:
    enabled: false  # D√©sactiv√© (balanced = pas self-consistency)
    method: "self_consistency"
    num_samples: 3
    agreement_threshold: 0.8
    action_on_failure: "regenerate"

  # Stage 4 : Completeness (Couverture question)
  completeness:
    enabled: true
    method: "llm"  # llm/heuristic

    llm_config:
      provider: "ollama"
      model: "llama3"
      prompt_template: |
        Question: {query}
        Answer: {answer}

        Does the answer fully address the question?
        Answer: YES or NO

    action_on_failure: "warn"

  # Stage 5 : Relevance (Pertinence)
  relevance:
    enabled: false  # D√©sactiv√© (balanced = faithfulness suffit)
    method: "semantic_similarity"
    model: "sentence-transformers/all-MiniLM-L6-v2"
    min_similarity: 0.75
    action_on_failure: "warn"

  # Actions globales
  overall_action:
    on_any_failure: "warn"    # warn/reject/regenerate
    on_all_pass: "return"

# =============================================================================
# [5.9] RESPONSE REFINEMENT (ITERATIVE)
# =============================================================================

response_refinement:
  enabled: false  # D√©sactiv√© (balanced = trop lent)
  description: "Raffinement it√©ratif avec self-critique"

  max_iterations: 2
  improvement_threshold: 0.05  # +5% qualit√© minimum pour continuer

  # Self-critique
  self_critique:
    enabled: false
    aspects:
      - "factuality"
      - "completeness"
      - "clarity"
      - "citation_quality"

    critique_prompt: |
      Evaluate your answer according to:

      1. Factuality: Is everything factual and grounded?
      2. Completeness: Does it fully answer the question?
      3. Clarity: Is it clear and well-structured?
      4. Citations: Are sources properly cited?

      For each aspect, rate 1-5 and suggest improvements.

  # Regeneration
  regenerate:
    enabled: false
    incorporate_critique: true
    temperature: 0.1  # L√©g√®rement plus cr√©atif pour variation

  # Validation d'am√©lioration
  validate_improvement:
    enabled: false
    metric: "faithfulness"  # faithfulness/quality_score
    min_improvement: 0.05

# =============================================================================
# [5.10] POST-PROCESSING & FORMATTING
# =============================================================================

post_processing:
  enabled: true
  description: "Formatage final et m√©tadonn√©es"

  # Formatage de sortie
  formatting:
    output_format: "markdown"  # markdown/json/text/html

    # Markdown config
    markdown_config:
      use_headers: false
      use_bold: true
      use_lists: true

    # JSON config
    json_config:
      schema: |
        {
          "answer": "string",
          "confidence": "float",
          "citations": ["array of doc_ids"],
          "sources": ["array of source info"],
          "metadata": {
            "latency_ms": "int",
            "tokens_used": "int"
          }
        }

  # Liste des sources
  sources_list:
    append: true
    format: "compact"  # compact/detailed

    template_compact: |

      ---
      Sources : {source_list}

    template_detailed: |

      ---
      Sources utilis√©es :
      {sources_detailed}

  # M√©tadonn√©es
  metadata:
    include: true
    fields:
      - "latency_ms"
      - "tokens_prompt"
      - "tokens_completion"
      - "cost_usd"
      - "faithfulness_score"
      - "hallucination_detected"
      - "validation_passed"

  # Nettoyage
  cleanup:
    clean_whitespace: true
    remove_duplicate_citations: true
    fix_citation_format: true

  # Audit trail (pour high-stakes)
  audit_trail:
    enabled: false  # D√©sactiv√© (balanced = pas n√©cessaire)
    log_all_steps: false
    include_intermediate_outputs: false

# =============================================================================
# üîß PROVIDERS & MODELS
# =============================================================================

providers:
  # LLM providers
  ollama:
    host: "http://localhost:11434"
    models:
      llama3:
        context_window: 8192
        cost_per_1m_tokens: 0.0  # Gratuit
      llama3_1_70b:
        context_window: 131072
        cost_per_1m_tokens: 0.0

  openai:
    api_key: "${OPENAI_API_KEY}"
    models:
      gpt4o_mini:
        name: "gpt-4o-mini"
        context_window: 128000
        cost_per_1m_input: 0.15
        cost_per_1m_output: 0.60
        structured_output: true

  anthropic:
    api_key: "${ANTHROPIC_API_KEY}"
    models:
      claude_3_5_sonnet:
        name: "claude-3-5-sonnet-20241022"
        context_window: 200000
        cost_per_1m_input: 3.0
        cost_per_1m_output: 15.0

  # Hallucination detection
  lettucedetect:
    library: "transformers"
    model: "adaamko/lettucedetect"
    install: "pip install transformers"

  cleanlab_tlm:
    library: "cleanlab"
    api_key: "${CLEANLAB_API_KEY}"
    install: "pip install cleanlab"

  # Validation models
  ragas:
    library: "ragas"
    install: "pip install ragas"

  # Structured output frameworks
  guidance:
    library: "guidance"
    install: "pip install guidance"

  outlines:
    library: "outlines"
    install: "pip install outlines"

# =============================================================================
# üìä M√âTRIQUES & MONITORING
# =============================================================================

metrics:
  enabled: true

  # M√©triques de qualit√©
  quality_metrics:
    - name: "answer_quality"
      description: "Qualit√© globale de la r√©ponse (0-1)"
      target: 0.75
      critical: true

    - name: "faithfulness_score"
      description: "Fid√©lit√© au contexte (RAGAS)"
      target: 0.86
      critical: true

    - name: "hallucination_rate"
      description: "% r√©ponses avec hallucinations"
      target: 0.11
      critical: true

    - name: "attribution_accuracy"
      description: "% citations correctes"
      target: 0.80

    - name: "citation_recall"
      description: "% sources cit√©es / sources utilis√©es"
      target: 0.85

  # M√©triques de performance
  performance_metrics:
    - name: "latency_ms"
      description: "Latence totale g√©n√©ration"
      target: 3800
      alert_threshold: 5000

    - name: "tokens_prompt"
      description: "Tokens input"
      aggregation: "mean"

    - name: "tokens_completion"
      description: "Tokens output"
      aggregation: "mean"

    - name: "cost_per_query"
      description: "Co√ªt par requ√™te (USD)"
      target: 0.012

  # Monitoring
  monitoring:
    log_metrics: true
    log_per_step: true

    alerts:
      latency_exceeded_ms: 5000
      hallucination_rate_high: 0.15
      faithfulness_low: 0.80

# =============================================================================
# üêõ DEBUG & LOGGING
# =============================================================================

debug:
  enabled: false
  log_level: "INFO"  # DEBUG/INFO/WARN/ERROR

  log_components:
    - "pre_generation_analysis"
    - "crag_evaluator"
    - "prompt_construction"
    - "initial_generation"
    - "self_rag"
    - "hallucination_detection"
    - "validation"

  save_prompts: false
  save_responses: false

  latency_alerts:
    pre_generation: 200
    prompt_construction: 50
    initial_generation: 2000
    self_rag: 1000
    hallucination_detection: 200
    validation: 250
    post_processing: 100

# =============================================================================
# üß™ TESTS & VALIDATION
# =============================================================================

tests:
  unit_tests:
    test_query_complexity_classification: true
    test_crag_evaluator: true
    test_hallucination_detection: true
    test_faithfulness_validation: true

  integration_tests:
    test_full_pipeline_simple: true
    test_full_pipeline_complex: true
    test_self_rag_activation: true

  evaluation:
    enabled: false
    datasets:
      - "squad"
      - "naturalquestions"
      - "hotpotqa"

    metrics:
      - "answer_quality"
      - "faithfulness"
      - "hallucination_rate"
      - "latency"

# =============================================================================
# üìù NOTES D'UTILISATION
# =============================================================================
#
# D√âMARRAGE RAPIDE :
#
# Cette configuration "balanced" active :
# ‚úÖ [5.1] Pre-Generation Analysis (CRAG + query complexity)
# ‚úÖ [5.2] Prompt Construction Adaptive
# ‚ö†Ô∏è [5.3] Advanced Prompting (CoT si complex)
# ‚úÖ [5.4] Initial Generation
# ‚ö†Ô∏è [5.5] Self-RAG (si query ambig√ºe)
# ‚ùå [5.6] GINGER (d√©sactiv√©, document-level citations)
# ‚úÖ [5.7] Hallucination Detection (LettuceDetect + TLM)
# ‚úÖ [5.8] Multi-Stage Validation (Faithfulness + Attribution + Completeness)
# ‚ùå [5.9] Response Refinement (d√©sactiv√©, trop lent)
# ‚úÖ [5.10] Post-Processing
#
# GAINS ATTENDUS :
# - Qualit√© : +15% (65% ‚Üí 75%)
# - Faithfulness : +10% (0.78 ‚Üí 0.86)
# - Hallucinations : -40% (18% ‚Üí 11%)
# - Latence : +52% (2.5s ‚Üí 3.8s)
# - Co√ªt : +25%
#
# INSTALLATION :
#
# pip install ragas cleanlab transformers guidance
#
# CONFIGURATION AVANC√âE :
#
# Pour activer GINGER (claim-level citations) :
#   grounded_generation.enabled = true
#   grounded_generation.nugget_extraction.enabled = true
#
# Pour activer Response Refinement (high-stakes) :
#   response_refinement.enabled = true
#   response_refinement.max_iterations = 2
#
# Pour activer Self-Consistency :
#   advanced_prompting.self_consistency.enabled = true
#   advanced_prompting.self_consistency.num_samples = 3
#
# D√âPENDANCES :
#
# - Phase 01 : Query understanding pour query_type classification
# - Phase 04 : Context compress√© en entr√©e
# - global.yaml : LLM providers (ollama, openai, anthropic)
#
# =============================================================================
