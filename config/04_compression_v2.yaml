# =============================================================================
# √âTAPE 04 v2 : COMPRESSION CONTEXTUELLE - CONFIGURATION COMPL√àTE
# =============================================================================
# Configuration d√©taill√©e et enrichie de l'√©tape de compression contextuelle.
#
# NOUVEAUT√âS v2 :
# - [4.1] Pre-Compression Analysis : Analyse de complexit√© et compressibilit√©
# - [4.2] Selective Compression (RECOMP) : Compression s√©lective extractive/abstractive
# - [4.3] Prompt Compression (LLMLingua) : Compression agressive de prompts (4x-20x)
# - [4.4] Contextual Compression : Compression contextuelle adaptative
# - [4.5] Token-Level Compression : Compression au niveau token (exp√©rimental)
# - [4.6] MMR with Compression Awareness : MMR intelligent avec compression
# - [4.7] Quality Validation Post-Compression : Validation qualit√© stricte
# - [4.8] Context Window Optimization : Gestion intelligente du context window
#
# GAINS ATTENDUS v2 :
# - Qualit√© : +15-35% selon configuration
# - Compression : 2.5x √† 10x selon strat√©gie
# - Co√ªts : -20% √† -90% tokens
# - Latence : +200ms √† +900ms selon features activ√©es
# =============================================================================

# =============================================================================
# üìã CONFIGURATION PRINCIPALE
# =============================================================================

step_04_compression:
  enabled: true
  version: "v2"

  # Configuration "balanced" activ√©e par d√©faut
  # √âquilibre qualit√©/co√ªt/latence optimal
  # Compression 2.5x, +15% qualit√©, -20% co√ªts, +385ms latence

  description: "Compression contextuelle intelligente avec LLMLingua et validation qualit√©"

  # M√©triques cibles
  targets:
    compression_ratio: 2.5  # 2.5x compression
    quality_gain: 0.15      # +15%
    cost_reduction: 0.20    # -20% tokens
    latency_budget_ms: 385  # Budget latence total

  # Pipeline d'ex√©cution s√©quentiel
  pipeline:
    - step: "pre_compression_analysis"
      enabled: true
      latency_budget_ms: 25

    - step: "selective_compression"
      enabled: false  # D√©sactiv√© (trop agressif pour balanced)
      latency_budget_ms: 0

    - step: "prompt_compression_llmlingua"
      enabled: true
      latency_budget_ms: 200

    - step: "contextual_compression"
      enabled: true
      latency_budget_ms: 120

    - step: "token_level_compression"
      enabled: false  # Exp√©rimental
      latency_budget_ms: 0

    - step: "mmr_compression_aware"
      enabled: true
      latency_budget_ms: 40

    - step: "quality_validation"
      enabled: true
      latency_budget_ms: 40

    - step: "context_window_optimization"
      enabled: true
      latency_budget_ms: 20

# =============================================================================
# [4.1] PRE-COMPRESSION ANALYSIS
# =============================================================================

pre_compression_analysis:
  enabled: true
  description: "Analyse de complexit√© et compressibilit√© avant compression"

  # Analyse de complexit√© du contenu
  complexity_analysis:
    enabled: true
    metrics:
      - "info_density"              # Densit√© informationnelle
      - "vocabulary_diversity"      # Diversit√© vocabulaire
      - "sentence_length_variance"  # Variance longueur phrases
      - "entity_density"            # Densit√© entit√©s nomm√©es

    scoring:
      method: "weighted_average"
      weights:
        info_density: 0.35
        vocabulary_diversity: 0.25
        sentence_length_variance: 0.15
        entity_density: 0.25

    thresholds:
      simple: 0.3      # Score < 0.3 : contenu simple
      medium: 0.6      # Score 0.3-0.6 : contenu moyen
      complex: 1.0     # Score > 0.6 : contenu complexe

  # Score de compressibilit√© (facilit√© de compression)
  compressibility_score:
    enabled: true
    method: "entropy"  # entropy/repetition_ratio/redundancy

    entropy_config:
      window_size: 100        # Fen√™tre d'analyse tokens
      normalization: true     # Normaliser score [0-1]

    thresholds:
      highly_compressible: 0.3    # Entropie < 0.3
      moderately_compressible: 0.6
      poorly_compressible: 1.0

  # Densit√© d'entit√©s nomm√©es
  entity_density:
    enabled: true
    ner_model: "fr_core_news_md"  # spaCy model
    entity_types:
      - "PERSON"
      - "ORG"
      - "GPE"
      - "DATE"
      - "MONEY"
      - "PERCENT"

    density_calculation:
      method: "entities_per_100_tokens"
      threshold_high: 8.0  # > 8 entit√©s / 100 tokens = dense

  # D√©tection de redondance inter-documents
  redundancy_detection:
    enabled: true
    method: "semantic_similarity"  # semantic_similarity/ngram_overlap

    semantic_similarity_config:
      model: "sentence-transformers/all-MiniLM-L6-v2"
      threshold: 0.85  # Similarit√© > 0.85 = redondant
      window_size: 3   # Comparer par groupes de 3 docs

    ngram_overlap_config:
      n: 3  # Trigrams
      threshold: 0.6  # Overlap > 60% = redondant

  # Sortie
  output:
    add_to_metadata: true  # Ajouter scores aux metadata
    log_analysis: true
    metrics_to_track:
      - "complexity_score"
      - "compressibility_score"
      - "entity_density"
      - "redundancy_ratio"

# =============================================================================
# [4.2] SELECTIVE COMPRESSION (RECOMP)
# =============================================================================
# REtrieval, COmpression, and Prependingt
# Fan et al. (2024) - Compression 10x-20x (5-10% tokens)
# √âtat : D√âSACTIV√â dans preset "balanced" (trop agressif)

selective_compression:
  enabled: false
  description: "Compression s√©lective RECOMP - extractive et abstractive"

  # S√©lection extractive de phrases (rapide)
  extractive:
    enabled: true
    method: "relevance_scoring"  # relevance_scoring/sentence_transformer

    relevance_scoring:
      scorer_model: "BAAI/bge-m3"
      max_sentences_per_doc: 3     # Garder top-3 phrases/doc
      relevance_threshold: 0.7     # Score min pour garder phrase
      query_aware: true            # Scorer selon query

    sentence_selection:
      prioritize_first_sentence: true   # Toujours garder 1√®re phrase
      prioritize_last_sentence: false
      preserve_question_answers: true   # Phrases avec "?" importantes

  # R√©sum√© abstractif (lent mais meilleur)
  abstractive:
    enabled: false  # Trop lent pour balanced
    model: "t5-base"  # t5-base/facebook/bart-large-cnn
    target_length: 100  # Tokens par document

    generation_config:
      max_length: 150
      min_length: 50
      temperature: 0.7
      top_p: 0.9
      num_beams: 4

    query_conditioning:
      enabled: true  # Conditionner r√©sum√© sur query
      prompt_template: "Summarize the following document focusing on: {query}\n\nDocument: {document}"

  # Mode hybride (extractive + abstractive)
  hybrid:
    enabled: false
    strategy: "extractive_first"  # extractive_first/abstractive_first

    extractive_first_config:
      extractive_ratio: 0.6  # 60% du contenu
      abstractive_ratio: 0.4

    fusion_method: "concatenate"  # concatenate/interleave

# =============================================================================
# [4.3] PROMPT COMPRESSION (LLMLingua)
# =============================================================================
# Microsoft Research - LLMLingua family
# LLMLingua-2 : 2.5x-4x compression, +21.4% performance
# LongLLMLingua : meilleur pour long context
# √âtat : ACTIV√â avec LLMLingua-2 (balanced)

prompt_compression:
  enabled: true
  description: "Compression agressive de prompts avec LLMLingua"

  # Choix du mod√®le LLMLingua
  tool: "llmlingua2"  # llmlingua/longllmlingua/llmlingua2

  # Configuration LLMLingua-2 (recommand√©, plus rapide)
  llmlingua2:
    compression_rate: 0.4  # 2.5x compression (0.4 = garder 40% tokens)

    # Pr√©servation s√©lective
    preserve_named_entities: true      # Garder entit√©s
    preserve_numbers: true             # Garder chiffres
    preserve_structure: false          # false = plus agressif
    preserve_question: true            # Toujours garder query compl√®te

    # Contr√¥leur de budget
    budget_controller:
      enabled: true
      target_context_size: 4000        # Tokens max contexte
      dynamic_compression: true        # Ajuster compression si overflow
      min_compression_rate: 0.2        # Minimum 20% gard√©s
      max_compression_rate: 0.6        # Maximum 60% gard√©s

    # Configuration mod√®le
    model_config:
      model_name: "microsoft/llmlingua-2-xlm-roberta-large-meetingbank"
      device: "cpu"  # cpu/cuda
      max_batch_size: 16

  # Configuration LongLLMLingua (meilleur pour long context)
  longllmlingua:
    compression_rate: 0.25  # 4x compression

    question_aware: true               # Compression selon query
    boost_question_proximity: true     # Boost passages pr√®s des mots-cl√©s query
    proximity_window: 50               # Tokens autour mot-cl√©

    coarse_to_fine:
      enabled: true                    # Compression 2 passes
      coarse_compression_rate: 0.5     # 1√®re passe : 2x
      fine_compression_rate: 0.25      # 2√®me passe : 4x total

    model_config:
      model_name: "NousResearch/Llama-2-7b-hf"
      device: "cpu"
      max_length: 4096

  # Configuration LLMLingua original
  llmlingua:
    compression_rate: 0.3
    target_token: -1  # Auto

    iterative_compression:
      enabled: true
      max_iterations: 3
      convergence_threshold: 0.05

    model_config:
      model_name: "gpt2"
      device: "cpu"

  # Post-traitement
  post_processing:
    remove_duplicate_tokens: true
    fix_broken_words: true
    restore_punctuation: true

  # M√©triques
  metrics:
    track_compression_ratio: true
    track_latency: true
    track_quality_loss: true
    alert_on_excessive_compression: true
    max_acceptable_compression: 10.0  # Alerte si > 10x

# =============================================================================
# [4.4] CONTEXTUAL COMPRESSION
# =============================================================================

contextual_compression:
  enabled: true
  description: "Compression contextuelle adaptative extractive"

  method: "extractive"  # extractive/abstractive/llm_based

  # Compression extractive (rapide, pr√©servation qualit√©)
  extractive:
    scorer_model: "BAAI/bge-m3"

    # Longueur passage adaptative
    max_passage_length: 200  # Tokens par passage
    min_passage_length: 50

    adaptive_passage_length:
      enabled: true
      by_complexity:
        simple: 100      # Contenu simple : passages courts
        medium: 200
        complex: 300     # Contenu complexe : passages longs

      by_query_type:
        factual: 100
        analytical: 250
        comparative: 200

    # Scoring de relevance
    relevance_threshold: 0.4  # Score min pour garder passage
    scoring_method: "cosine_similarity"  # cosine_similarity/cross_encoder

    cross_encoder_config:
      model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
      batch_size: 32

    # Fen√™tre contextuelle
    context_window:
      before_tokens: 20  # Tokens avant passage relev√©
      after_tokens: 20   # Tokens apr√®s passage relev√©

  # Compression abstractive (LLM-based)
  abstractive:
    enabled: false
    method: "summarization"  # summarization/paraphrasing

    model: "facebook/bart-large-cnn"
    target_compression: 0.5  # 50% de la longueur

    generation_config:
      max_length: 200
      min_length: 50
      temperature: 0.7

  # Multi-document summary (cross-document compression)
  multi_doc_summary:
    enabled: false  # D√©sactiv√© (latence)
    method: "clustering"  # clustering/graph_based

    clustering_config:
      algorithm: "kmeans"
      n_clusters: 5
      summarize_per_cluster: true

    graph_based_config:
      algorithm: "textrank"
      damping: 0.85
      max_iter: 100

# =============================================================================
# [4.5] TOKEN-LEVEL COMPRESSION (EXP√âRIMENTAL)
# =============================================================================

token_level_compression:
  enabled: false  # Exp√©rimental, d√©sactiv√© par d√©faut
  description: "Compression au niveau token avec classification"

  method: "classification"  # classification/importance_scoring

  # Classification binaire (garder/supprimer)
  token_classification:
    enabled: false
    model: "bert-base-uncased"

    classifier_config:
      threshold: 0.5  # Prob > 0.5 = garder
      batch_size: 64

    training_data:
      use_synthetic: true
      synthetic_method: "mask_and_predict"

  # Importance scoring
  importance_scoring:
    enabled: false
    method: "attention"  # attention/gradient/tf_idf

    attention_config:
      layer: -1  # Derni√®re couche
      head: "mean"  # mean/max
      threshold: 0.3  # Score min

    gradient_config:
      target_layer: -1
      threshold: 0.2

    tf_idf_config:
      vocab_size: 10000
      threshold: 0.1

# =============================================================================
# [4.6] MMR WITH COMPRESSION AWARENESS
# =============================================================================

mmr_compression_aware:
  enabled: true
  description: "MMR intelligent avec awareness de la compression"

  # Lambda adaptatif selon query type
  adaptive_lambda:
    enabled: true

    by_query_type:
      factual: 0.7        # Privil√©gier relevance
      analytical: 0.5     # √âquilibre
      comparative: 0.6
      opinion: 0.4        # Privil√©gier diversit√©

    default: 0.6

    source:
      provider: "phase_01"  # phase_01/local_classifier
      fallback: 0.6

  # Compression awareness
  compression_aware:
    enabled: true

    # Boost documents bien compress√©s
    boost_well_compressed:
      enabled: true
      compression_ratio_threshold: 2.0  # Docs compress√©s > 2x
      boost_factor: 1.1  # +10% score

    # P√©naliser perte de compression
    compression_loss_weight: 0.2  # 20% du score

    scoring_formula: "relevance * (1 + boost) - (compression_loss * loss_weight)"

  # Top-K final
  final_top_k: 15  # Nombre de documents apr√®s MMR

  # Diversit√©
  diversity_threshold: 0.85  # Similarit√© max entre docs s√©lectionn√©s

# =============================================================================
# [4.7] QUALITY VALIDATION POST-COMPRESSION
# =============================================================================

quality_validation:
  enabled: true
  description: "Validation qualit√© stricte post-compression"

  # Similarit√© s√©mantique (original vs compress√©)
  semantic_similarity:
    enabled: true
    model: "sentence-transformers/all-MiniLM-L6-v2"

    min_similarity: 0.85  # Seuil minimum acceptable

    action: "warn"  # warn/reject/recompress

    comparison_method: "sentence_embeddings"  # sentence_embeddings/paragraph_embeddings

  # Pr√©servation d'entit√©s
  entity_preservation:
    enabled: true
    ner_model: "fr_core_news_md"

    min_coverage: 0.9  # 90% entit√©s pr√©serv√©es minimum

    entity_types_critical:
      - "PERSON"
      - "ORG"
      - "DATE"
      - "MONEY"

    action: "warn"

  # Couverture de la r√©ponse (answerability)
  answer_coverage:
    enabled: true
    method: "llm"  # llm/rule_based

    llm_config:
      provider: "ollama"
      model: "llama3"

      prompt_template: |
        Question: {query}

        Context compress√©: {compressed_context}

        Le contexte compress√© contient-il suffisamment d'information pour r√©pondre √† la question ?
        R√©pondre par OUI ou NON uniquement.

      min_confidence: 0.8

    action: "recompress"  # recompress/reject/warn

  # V√©rification ratio de compression
  compression_ratio_check:
    enabled: true

    min_ratio: 1.5   # Compression minimale 1.5x
    max_ratio: 10.0  # Compression maximale 10x

    target_ratio: 2.5  # Cible

    action: "adjust"  # adjust/warn

  # Recompression si √©chec validation
  recompression:
    enabled: true
    max_attempts: 2  # Nombre tentatives max

    strategy: "reduce_compression_rate"  # reduce_compression_rate/use_fallback_method

    rate_reduction: 0.1  # -10% compression chaque tentative

# =============================================================================
# [4.8] CONTEXT WINDOW OPTIMIZATION
# =============================================================================

context_window_optimization:
  enabled: true
  description: "Gestion intelligente du context window"

  # Taille maximale du contexte
  max_context_tokens: 100000  # Limite absolue

  target_context_tokens: 4000  # Cible pour g√©n√©ration

  # Allocation dynamique des tokens
  dynamic_allocation:
    enabled: true

    allocation_strategy: "ranked"  # ranked/proportional/equal

    # Strat√©gie ranked : plus de tokens aux docs top-k
    ranked_config:
      top_k_boost: 1.5        # Top-5 re√ßoivent 50% plus tokens
      top_k_threshold: 5

      allocation_curve: "exponential"  # exponential/linear
      decay_factor: 0.9  # Doc N re√ßoit 90% de doc N-1

    # Strat√©gie proportional : selon score
    proportional_config:
      score_weight: 0.7
      position_weight: 0.3

  # Truncate intelligent
  smart_truncate:
    enabled: true

    priority: "relevance"  # relevance/recency/position

    preserve_top_k: 5  # Toujours garder top-5 complets

    truncate_method: "sentence_boundary"  # sentence_boundary/token_boundary

    preserve_first_last:
      enabled: true
      first_tokens: 100
      last_tokens: 50

  # Gestion du budget de tokens
  token_budget:
    strategy: "proportional"  # proportional/fixed/adaptive

    # R√©serve pour la r√©ponse
    reserve_for_answer: 2000  # Tokens pour g√©n√©ration r√©ponse

    # R√©serve pour l'historique conversation
    reserve_for_history: 500

    # Budget disponible pour contexte
    available_for_context: 3500  # max_context - reserves

  # Multi-turn awareness (conversation)
  multi_turn:
    enabled: false  # D√©sactiv√© (single-turn)

    history_compression:
      enabled: false
      method: "summarization"
      max_history_tokens: 500

# =============================================================================
# üîß PROVIDERS & MODELS
# =============================================================================

providers:
  # LLMLingua
  llmlingua:
    library: "llmlingua"
    install_command: "pip install llmlingua"

    models:
      llmlingua2:
        name: "microsoft/llmlingua-2-xlm-roberta-large-meetingbank"
        size: "560MB"
        performance: "best_quality_speed"

      longllmlingua:
        name: "NousResearch/Llama-2-7b-hf"
        size: "13GB"
        performance: "best_long_context"

      llmlingua:
        name: "gpt2"
        size: "500MB"
        performance: "baseline"

  # Sentence Transformers (embeddings)
  sentence_transformers:
    library: "sentence-transformers"

    models:
      bge_m3:
        name: "BAAI/bge-m3"
        size: "2.3GB"
        use_case: "scoring_relevance"

      minilm:
        name: "sentence-transformers/all-MiniLM-L6-v2"
        size: "80MB"
        use_case: "similarity_validation"

  # NER (Named Entity Recognition)
  spacy:
    library: "spacy"

    models:
      french:
        name: "fr_core_news_md"
        install_command: "python -m spacy download fr_core_news_md"
        size: "43MB"

  # LLM pour validation
  ollama:
    host: "http://localhost:11434"

    models:
      llama3:
        name: "llama3"
        use_case: "answer_coverage_validation"

  # Tokenizers
  tiktoken:
    library: "tiktoken"
    install_command: "pip install tiktoken"

    encoding: "cl100k_base"  # GPT-4/GPT-3.5-turbo

# =============================================================================
# üìä M√âTRIQUES & MONITORING
# =============================================================================

metrics:
  enabled: true

  # M√©triques de compression
  compression_metrics:
    - name: "compression_ratio"
      description: "Ratio de compression (tokens_avant / tokens_apr√®s)"
      target: 2.5

    - name: "tokens_saved"
      description: "Nombre de tokens √©conomis√©s"
      aggregation: "sum"

    - name: "cost_reduction"
      description: "R√©duction co√ªt en %"
      target: 0.20

  # M√©triques de qualit√©
  quality_metrics:
    - name: "semantic_similarity"
      description: "Similarit√© contexte original vs compress√©"
      target: 0.85
      critical: true

    - name: "entity_preservation_rate"
      description: "% entit√©s pr√©serv√©es"
      target: 0.90
      critical: true

    - name: "answer_coverage"
      description: "Question r√©pondable apr√®s compression"
      target: 1.0
      critical: true

  # M√©triques de performance
  performance_metrics:
    - name: "compression_latency_ms"
      description: "Latence totale compression"
      target: 385
      alert_threshold: 500

    - name: "throughput_docs_per_sec"
      description: "D√©bit documents/seconde"
      target: 10

  # Monitoring
  monitoring:
    log_metrics: true
    log_per_step: true

    alerts:
      compression_ratio_too_low: 1.5
      compression_ratio_too_high: 10.0
      quality_drop_threshold: 0.80
      latency_exceeded_ms: 500

# =============================================================================
# üêõ DEBUG & LOGGING
# =============================================================================

debug:
  enabled: false
  log_level: "INFO"  # DEBUG/INFO/WARN/ERROR

  log_substeps: true

  include_compression_stats: true

  latency_alerts:
    step_4_1: 25    # Pre-compression analysis
    step_4_2: 600   # RECOMP abstractive (tr√®s lent)
    step_4_3: 200   # LLMLingua
    step_4_4: 120   # Contextual compression
    step_4_5: 100   # Token-level
    step_4_6: 40    # MMR
    step_4_7: 40    # Validation
    step_4_8: 20    # Context window

  save_intermediate_outputs:
    enabled: false
    output_dir: "./debug/compression"

    save:
      - "pre_compression_documents"
      - "post_llmlingua_documents"
      - "post_contextual_documents"
      - "final_documents"

# =============================================================================
# üß™ TESTS & VALIDATION
# =============================================================================

tests:
  unit_tests:
    test_compression_ratio: true
    test_quality_preservation: true
    test_latency_budget: true

  integration_tests:
    test_full_pipeline: true
    test_with_phase_03_output: true

  benchmark:
    enabled: false

    datasets:
      - "msmarco"
      - "natural_questions"

    metrics:
      - "compression_ratio"
      - "semantic_similarity"
      - "answer_preservation"
      - "latency"

# =============================================================================
# üìù NOTES D'UTILISATION
# =============================================================================
#
# D√âMARRAGE RAPIDE :
#
# Cette configuration "balanced" active :
# ‚úÖ [4.1] Pre-Compression Analysis
# ‚ùå [4.2] Selective Compression (RECOMP) - D√©sactiv√© (trop agressif)
# ‚úÖ [4.3] Prompt Compression (LLMLingua-2) - 2.5x compression
# ‚úÖ [4.4] Contextual Compression - Extractive adaptatif
# ‚ùå [4.5] Token-Level Compression - D√©sactiv√© (exp√©rimental)
# ‚úÖ [4.6] MMR with Compression Awareness - Lambda adaptatif
# ‚úÖ [4.7] Quality Validation - Validation compl√®te
# ‚úÖ [4.8] Context Window Optimization - Allocation dynamique
#
# GAINS ATTENDUS :
# - Compression : 2.5x (40% tokens)
# - Qualit√© : +15%
# - Co√ªts : -20% tokens ‚Üí -20% co√ªt
# - Latence : +385ms
#
# INSTALLATION :
#
# pip install llmlingua sentence-transformers spacy tiktoken
# python -m spacy download fr_core_news_md
#
# CONFIGURATION AVANC√âE :
#
# Pour activer RECOMP (compression 10x-20x) :
#   selective_compression.enabled = true
#   selective_compression.extractive.enabled = true
#
# Pour compression ultra-agressive (cost_optimized) :
#   prompt_compression.llmlingua2.compression_rate = 0.2  # 5x
#   selective_compression.enabled = true
#
# Pour compression maximale (maximal) :
#   prompt_compression.tool = "longllmlingua"  # Meilleur long context
#   prompt_compression.longllmlingua.compression_rate = 0.25  # 4x
#   selective_compression.abstractive.enabled = true  # R√©sum√©s abstractifs
#
# D√âPENDANCES :
#
# - Phase 01 : Query understanding pour adaptive lambda
# - Phase 03 : Reranked documents en entr√©e
# - tiktoken : Comptage tokens
# - LLMLingua : Compression prompts
# - spaCy : NER pour validation entit√©s
#
# =============================================================================
