# =============================================================================
# ÉTAPE 01 : QUERY PROCESSING & EXPANSION (RAG ULTIME 2025)
# =============================================================================
# Pipeline de transformation et enrichissement de la requête utilisateur.
#
# OBJECTIFS :
# - Transformer la requête en plusieurs variantes sémantiques (3-5 queries)
# - Lever les ambiguïtés et identifier les intentions multiples
# - Générer des représentations vectorielles pour le retrieval
#
# TECHNIQUES D'EXPANSION :
# 1. Query Rewriting avec Feedback (RaFe) : reformulation adaptative
# 2. HyDE : génération de réponse hypothétique pour matching sémantique
# 3. Multi-Query Expansion : 3-5 variantes de la requête originale
# 4. Step-Back Prompting : question abstraite de niveau supérieur
#
# OUTILS RECOMMANDÉS :
# - Golden-Retriever : augmentation domaines spécialisés
# - LangChain QueryExpander : génération variantes sémantiques
# - RaFe : query rewriting avec feedback du retrieval
#
# PERFORMANCE CIBLE :
# - Latence : 50-100ms
# - Cache : activé pour requêtes fréquentes
# - Amélioration qualité : +30-40% vs requête simple
# =============================================================================


# -----------------------------------------------------------------------------
# MODÈLE D'EMBEDDING (DENSE RETRIEVAL - PHASE 2.1)
#
# Rôle : Convertit tout texte (documents et requêtes) en vecteurs numériques
# pour permettre la recherche sémantique dense.
#
# MODÈLE RECOMMANDÉ 2025 : BGE-M3
# - Dimensions : 1024
# - Support : Dense + Sparse + ColBERT dans un seul modèle
# - Multilingue : natif (100+ langues)
# - Quantization : binary/scalar pour compression 6-10×
# - Performance : SOTA sur benchmarks MTEB
# -----------------------------------------------------------------------------
embedding:
  # Fournisseur du modèle d'embedding.
  # Il doit être déclaré dans `global.yaml` sous `llm_providers`.
  # Options : "sentence_transformers" (local gratuit), "openai", "mistral_ai".
  provider: "sentence_transformers"

  # Nom/identifiant du modèle chez le fournisseur.
  # RECOMMANDÉ : "BAAI/bge-m3" (meilleur modèle multilingue 2025)
  # Alternatives : "text-embedding-3-small" (OpenAI), "bge-large-en-v1.5"
  model_name: "BAAI/bge-m3"

  # Dimensions du vecteur d'embedding
  embedding_dim: 1024

  # Quantization pour réduction mémoire et accélération
  # Options : "none", "binary", "scalar", "product"
  quantization: "binary"  # Compression 6-10× avec perte qualité minimale

  # Fine-tuning domaine (optionnel)
  # Mettre à true si le modèle a été fine-tuné sur vos données métier
  fine_tuned: false

# --- EXEMPLE DE CONFIGURATION ALTERNATIVE (OpenAI) ---
# embedding:
#   provider: "openai"
#   model_name: "text-embedding-3-small"


# -----------------------------------------------------------------------------
# EXPANSION DE REQUÊTE (QUERY EXPANSION) - RAG ULTIME 2025
#
# Rôle : Utilise un LLM pour générer 3-5 variantes de la requête utilisateur.
# Cela permet de couvrir plus d'angles, lever les ambiguïtés et améliorer
# le recall du retrieval.
#
# TECHNIQUES UTILISÉES :
# - Rewrite : reformulation pour plus de clarté
# - HyDE : génération document hypothétique pour matching sémantique
# - Multi-Query : 3-5 variantes explorant différentes intentions
# - Step-Back : question générale pour capturer le contexte
#
# PERFORMANCE :
# - Latence cible : 50-100ms
# - Nombre de variantes : 3-5 (configurable)
# - Amélioration recall : +30-40% vs requête simple
# -----------------------------------------------------------------------------
query_expansion:
  # Mettre à `false` pour désactiver complètement l'expansion de requête.
  enabled: true

  # Mettre à `true` pour garder en mémoire les requêtes déjà expansées et
  # accélérer les recherches répétées (latence réduite à ~5ms).
  cache_enabled: true

  # Nombre de variantes à générer au total (toutes techniques confondues)
  # Recommandé : 3-5 pour équilibre qualité/latence
  total_variants_target: 5

  # Latence maximale acceptable (ms) pour l'expansion complète
  max_latency_ms: 100

  # --- LLM UTILISÉ POUR L'EXPANSION ---
  # Ce LLM agit comme un "cerveau créatif". Il doit être rapide.
  llm:
    # Fournisseur du LLM (doit être défini dans `global.yaml`).
    # Options communes : "ollama" (local), "openai", "mistral_ai", "openrouter".
    provider: "ollama"

    # Modèle de chat à utiliser.
    # Exemple pour "ollama": "llama3", "mistral"
    # Exemple pour "openai": "gpt-4o-mini"
    model: "llama3"

    # --- EXEMPLE DE CONFIGURATION ALTERNATIVE (OpenAI) ---
    # llm:
    #   provider: "openai"
    #   model: "gpt-4o-mini"

    # Degré de créativité du LLM.
    # 0.0 = réponses prévisibles et factuelles (recommandé).
    # 1.0 = réponses très créatives.
    temperature: 0.0

    # Longueur maximale de la réponse générée par le LLM.
    max_tokens: 150

  # --- TECHNIQUES D'EXPANSION ---
  # Chaque technique a un but précis. Activez celles qui sont pertinentes
  # pour votre cas d'usage.
  techniques:
    # Reformule la question pour plus de clarté.
    rewrite:
      enabled: true
    # Génère un document "idéal" pour trouver des contenus similaires.
    hyde:
      enabled: true
    # Crée plusieurs variantes de la question pour élargir la recherche.
    multi_query:
      enabled: true
      num_variants: 4 # Nombre de variantes à générer.
    # Formule une question plus générale pour trouver du contexte.
    step_back:
      enabled: true

  # --- PROMPTS ---
  # Instructions envoyées au LLM pour chaque technique.
  # Adaptez le ton et les instructions à votre domaine et à votre audience.
  prompts:
    hyde: |
      Vous êtes un expert du domaine. Rédigez un document concis qui répond de manière exhaustive à la question suivante. Ce document sera utilisé pour trouver des informations similaires. Ne mentionnez pas que c'est un document hypothétique.

      Question : {query}
    multi_query: |
      Générez {num_variants} reformulations alternatives de la question suivante, en explorant différentes intentions et points de vue. Listez chaque reformulation sur une nouvelle ligne, sans numérotation.

      Question originale : {query}
    step_back: |
      Quelle est la question plus générale et conceptuelle qui se cache derrière la question spécifique suivante ? Formulez uniquement la question générale.

      Question spécifique : {query}
    rewrite: |
      En tenant compte de l'historique de recherche (non implémenté pour l'instant), reformulez la requête suivante pour améliorer la pertinence des résultats.

      Requête originale : {query}
